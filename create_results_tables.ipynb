{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from utils.classificaton_utils import evaluate_results\n",
    "\n",
    "avg = \"macro\" # either macro or none (if none, we consider label 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 2: Main evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1850 = evaluate_results(Path(f'results_1850'),avg=avg)\n",
    "results_1920 = evaluate_results(Path(f'results_1920'),avg=avg)\n",
    "results_2000 = evaluate_results(Path('results_2000'),avg=avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1850 = pd.DataFrame.from_dict(results_1850, orient='index', columns=['precision','recall','fscore','preds'])\n",
    "df_1920 = pd.DataFrame.from_dict(results_1920, orient='index', columns=['precision','recall','fscore','preds'])\n",
    "df_2000 = pd.DataFrame.from_dict(results_2000, orient='index', columns=['precision','recall','fscore','preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fscores = pd.concat([df_1850[['precision', 'recall', 'fscore']], df_1920[['precision', 'recall', 'fscore']], df_2000[['precision', 'recall', 'fscore']]],axis=1) #Â ,df_2000['fscore']\n",
    "cols_baselines = [\"random\", \"def_tok_overlap_ranking\", \"sent_embedding\", \"w2v_lesk_ranking\", \"svm_wemb_baseline\"]\n",
    "cols_bert = [c for c in df_fscores.index if not 'ts' in c or 'contrast' in c]\n",
    "df_fscores = df_fscores.loc[cols_baselines + cols_bert]\n",
    "print(df_fscores.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_baselines + cols_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_1850 = [r for r in df_fscores.index if '1850' in r]\n",
    "rows_1920 = [r for r in df_fscores.index if 'blert' in r]\n",
    "rows_2000 = [r for r in df_fscores.index if 'bert_base' in r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_1850_2000 = df_1850.loc[df_1850.index.isin(rows_1850)].fscore.values - df_1850.loc[df_1850.index.isin(rows_2000)].fscore.values\n",
    "diff_1920_2000 = df_1920.loc[df_1920.index.isin(rows_1920)].fscore.values - df_1920.loc[df_1920.index.isin(rows_2000)].fscore.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff = pd.DataFrame([diff_1850_2000,diff_1920_2000],columns=rows_2000).T\n",
    "#df_diff['sum'] = df_diff.sum(axis=1)\n",
    "print(df_diff.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 3: Time-sensitive methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ts_1850 = evaluate_results(Path(\"results_ts_1850\"),avg=avg)\n",
    "results_ts_1920 = evaluate_results(Path(\"results_ts_1920\"),avg=avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts_1850 = pd.DataFrame.from_dict(results_ts_1850, orient='index', columns=['precision','recall','fscore','preds'])\n",
    "df_ts_1920 = pd.DataFrame.from_dict(results_ts_1920, orient='index', columns=['precision','recall','fscore','preds'])\n",
    "df_fscores = pd.concat([df_ts_1850['fscore'], df_ts_1920['fscore']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_fscores.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curated examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_curated_seed = evaluate_results(Path(f'results_cuevaluate_results'),avg=avg)\n",
    "results_curated_synonym = evaluate_results(Path(f'results_curated_1920_syn'),avg=avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_curated_seed = pd.DataFrame.from_dict(results_curated_seed, orient='index', columns=['precision','recall','fscore','preds'])\n",
    "df_curated_synonym = pd.DataFrame.from_dict(results_curated_synonym, orient='index', columns=['precision','recall','fscore','preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fscore = pd.concat([df_curated_seed['fscore'],df_curated_synonym['fscore']], axis=1)\n",
    "df_fscore.columns=['vertical','horizontal']\n",
    "print(df_fscore.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess Statistical Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = evaluate_results(Path(\"results_1850/\"),avg=avg)\n",
    "\n",
    "selected = \"bert_binary_centroid_vector_bert_base_-1,-2,-3,-4_mean\"\n",
    "\n",
    "selected_pred = res[selected][3][0]\n",
    "print (selected, res[selected][:3], \"\\n\\nIs the difference significant?\\n\")\n",
    "\n",
    "for method,values in res.items():\n",
    "    if method != selected:\n",
    "        pred = values[3][0]\n",
    "        p_value = scipy.stats.ttest_rel(selected_pred,pred)[1]\n",
    "        if p_value<0.05:\n",
    "            print (method, values[:3], \"YES\")\n",
    "        else:\n",
    "            print (method, values[:3], \"NO p_value:\",round(p_value,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "py37deezy",
   "display_name": "Python (py37deezy)",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}