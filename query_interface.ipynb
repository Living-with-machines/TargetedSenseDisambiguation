{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset_download import *\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path, PosixPath\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import API credentials\n",
    "with open('oed_experiments/oed_credentials.json') as f:\n",
    "    credentials = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lemma\n",
    "lemma_id = \"machine_nn01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path(\"./data\")\n",
    "save_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query the API and get the json response\n",
    "sense_json = query_oed(credentials,'word',lemma_id,flags='include_senses=true&include_quotations=true')\n",
    "\n",
    "# convert the json in a dataframe\n",
    "senses_df = convert_json_to_dataframe(sense_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe\n",
    "# as pickle\n",
    "senses_df.to_pickle(save_path / f\"senses_{lemma_id}.pickle\")\n",
    "# as csv\n",
    "senses_df.to_csv(save_path / f\"senses_{lemma_id}.tsv\",sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open pickle file to avoid calling the API again\n",
    "with open(save_path / f\"senses_{lemma_id}.pickle\",'rb') as in_pickle:\n",
    "    machine_senses_df = pickle.load(in_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all senses that are siblings and descendants\n",
    "# of the semantic class of senses listed in previously obtained query \n",
    "responses = traverse_thesaurus(credentials,machine_senses_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traverse tree or load responses \n",
    "# responses = traverse_thesaurus(credentials,machine_senses_df)\n",
    "with open('./data/tree_traversal.pickle','rb') as in_pickle:\n",
    "    responses = pickle.load(in_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all quoations for the senses in the responses variable\n",
    "quotations = get_quotations_from_thesaurus(credentials,responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge and save all information stored in the seperate pickle files\n",
    "df = merge_pickled(Path(\"./data/senses_machine_nn01.pickle\"),\n",
    "                   Path(\"./data/tree_traversal.pickle\"),\n",
    "                   Path(\"./data/tree_traversal_quotations.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(f\"./data/{lemma_id}_all.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start,end = 1750,1950\n",
    "lemma_id = 'machine_nn01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_last_id = lambda nested_list :[l[-1] for l in nested_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_from_lemma_query(auth,lemma_id,start=1750,end=1950):\n",
    "    \"\"\"Extends senses from a dataframe generate from accessing\n",
    "    the API via the word endpoint. The script first retrieves all\n",
    "    senses, then synonyms for these senses, then other senses that \n",
    "    match the semantic classes of the retrieved senses.\n",
    "    \n",
    "    This script also aims to record the \"provenance\" of words, \n",
    "    their relation to the initial query, which can help to \n",
    "    select of filter words later on.\n",
    "    \n",
    "    Arguments:\n",
    "        lemma_id (str)\n",
    "        start (int)\n",
    "        end (int)\n",
    "    Returns\n",
    "        a pandas.DataFrame\n",
    "    \"\"\"\n",
    "    # load seed query dataframe\n",
    "    query_df = pd.read_pickle(f\"./data/senses_{lemma_id}.pickle\")\n",
    "    \n",
    "    # use the sense endpoint to ensure all information \n",
    "    # can be properly concatenated in one dataframe\n",
    "    \n",
    "    # retrieve all sense ids\n",
    "    query_sense_ids = query_df.id.unique()\n",
    "    \n",
    "    # get all senses by sense id\n",
    "    print(f\"Get all sense for the lemma {lemma_id}\")\n",
    "    seeds = [(s,query_oed(auth,'sense',s,\n",
    "                    flags=f\"current_in='{start}-{end}'&limit=1000\"))\n",
    "                        for s in tqdm(query_sense_ids)]\n",
    "    \n",
    "    # convert to dataframe\n",
    "    seeds_df = pd.DataFrame([seed['data'] for s_id,seed in seeds])\n",
    "    \n",
    "    # define provenance, these words are \"seed\"\n",
    "    seeds_df['provenance'] = seeds_df.id\n",
    "    seeds_df['provenance_type'] = 'seed'\n",
    "    \n",
    "    # get all synonyms for the seed senses\n",
    "    print(f\"Get all synonyms of the senses listed in {lemma_id}\")\n",
    "    synonyms = [(s,query_oed(auth,'sense',s,\n",
    "                    level='synonyms',\n",
    "                    flags=f\"current_in='{start}-{end}'&limit=1000\"))\n",
    "                            for s in tqdm(query_sense_ids)]\n",
    "\n",
    "    # transform list of synonyms to a dataframe\n",
    "    synonyms_df = pd.DataFrame([s for s_id,syn in synonyms for s in syn['data']])\n",
    "    \n",
    "    # these items have provenancy type \"synonym\"\n",
    "    synonyms_df['provenance'] = [s_id for s_id,syn in synonyms for s in syn['data']]\n",
    "    synonyms_df['provenance_type'] = 'synonym'\n",
    "    \n",
    "    # seed + synonyms constitute the nucleas of our query\n",
    "    # branch from there\n",
    "    core_df = pd.concat([seeds_df,synonyms_df])\n",
    "    core_df['semantic_class_last_id'] = core_df['semantic_class_ids'].apply(get_last_id)\n",
    "    \n",
    "    # retrieve all semantic class ids for the senses so far\n",
    "    semantic_class_ids = set([s for l in core_df.semantic_class_last_id.to_list() for s in l])\n",
    "\n",
    "    # get all the branches for the retrieve semantic class ids\n",
    "    print(\"Get all branches for seed senses and synonyms\")\n",
    "    branches = [(idx,query_oed(auth,'semanticclass', idx, \n",
    "                        level='branchsenses',\n",
    "                        flags=f\"current_in='{start}-{end}'&limit=1000\"))\n",
    "                            for idx in tqdm(semantic_class_ids)]\n",
    "    \n",
    "    # convert API response to dataframe\n",
    "    branches_df = pd.DataFrame([s for idx,branch in branches for s in branch['data']])\n",
    "    \n",
    "    # provenance_type is branch with semantic class id \n",
    "    # that was use for retrieving the sense is the provenance\n",
    "    branches_df['provenance'] = [idx for idx,branch in branches for s in branch['data']]\n",
    "    branches_df['provenance_type'] = 'branch'\n",
    "    \n",
    "    branches_df['semantic_class_last_id'] = branches_df.semantic_class_ids.apply(get_last_id)\n",
    "    \n",
    "    # remove senses that already appear in the core_df\n",
    "    branches_df_red = branches_df.loc[~branches_df.id.isin(core_df.id)]\n",
    "    \n",
    "    # concatenate core and branch senses\n",
    "    extended_df = pd.concat([core_df,branches_df_red])\n",
    "    \n",
    "    # refine the provenance type\n",
    "    # if the last semantic class id is not equal to provenance\n",
    "    # this row is a child or descendant\n",
    "    check_membership = lambda row : row.provenance in row.semantic_class_last_id\n",
    "    extended_df.loc[(~extended_df.apply(check_membership,axis=1)) & (extended_df.provenance_type=='branch'),\n",
    "                [\"provenance_type\"]]  = \"branch_descendant\"\n",
    "    \n",
    "    # save information\n",
    "    extended_df.to_pickle(f'./data/senses_{lemma_id}_extended.pickle')\n",
    "    return extended_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_df = extend_from_lemma_query(credentials,lemma_id,start,end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quotations(lemma_id,sense_df=''):\n",
    "    \"\"\"Obtain and store all quotations from a dataframe constructed\n",
    "    from information retrieved via de sense endpoint.\n",
    "    \n",
    "    Arguments:\n",
    "        sense_df (pandas.DataFrame,str)\n",
    "        lemma_id (str)\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame\n",
    "    \"\"\"\n",
    "    if isinstance(sense_df,str):\n",
    "        quotations_df = pd.read_pickle(f'./data/quotations_{lemma_id}.pickle')\n",
    "    else:\n",
    "        quotations = [query_oed(credentials,'sense',sense_idx,level='quotations')\n",
    "                        for sense_idx in tqdm(set(sense_df.id))]\n",
    "            \n",
    "        quotations_df = pd.concat([pd.DataFrame(q['data']) for q in quotations])\n",
    "        quotations_df.to_pickle(f'./data/quotations_{lemma_id}.pickle')\n",
    "    return quotations_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotations_df = get_quotations(lemma_id)\n",
    "quotations_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = extended_df.merge(quotations_df[['id','sense_id',\"text\",\"year\",\"source\"]],left_on='id',right_on='sense_id',suffixes=['',\"_quotation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging experiment with seed and synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_forms = set(extended_df[extended_df.provenance_type.isin(['seed','synonym'])].lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma_content_from_surface_form(surface_forms):\n",
    "    \"\"\"Get all quotations and senses from a list of surface forms\n",
    "    \"\"\"\n",
    "    lemmatized = [query_oed(credentials,'lemmatize','',flags=f\"form={sf}\")\n",
    "                             for sf in tqdm(surface_forms)]\n",
    "    \n",
    "    lemmas_content = [query_oed(credentials, 'word',\n",
    "                       l['word']['id'], \n",
    "                       flags=\"include_senses=true&include_quotations=true\") \n",
    "                             for lemma in tqdm(lemmatized)\n",
    "                                 for l in lemma['data']]\n",
    "    \n",
    "    lemmas_content_df = pd.DataFrame(sense for lemma in lemmas_content for sense in lemma['data']['senses'])\n",
    "    \n",
    "    quotations = [quotation\n",
    "                        for i,row in lemmas_content_df.iterrows()\n",
    "                             for quotation in row.quotations]\n",
    "    \n",
    "    related_quotations_df = pd.DataFrame(quotations)\n",
    "    \n",
    "    related_merged_df = lemmas_content_df.merge(related_quotations_df[['id','sense_id',\"text\",\"year\",\"source\"]],left_on='id',right_on='sense_id',suffixes=['',\"_quotation\"])\n",
    "    \n",
    "    related_merged_df['provenance'] = None\n",
    "    related_merged_df['provenance_type'] = 'related'\n",
    "    #related_merged_df.loc[related_merged_df.id.isin(merged_df.id),['provenance_type']] = 'in_core'\n",
    "\n",
    "    return related_merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start here if data is downloaded and put into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_merged_df = pd.read_pickle(f'./data/related_merged_{lemma_id}.pickle')\n",
    "merged_df = pd.read_pickle(f'./data/merged_{lemma_id}.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#related_merged_df.to_pickle(f'./data/related_merged_{lemma_id}.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_df = merged[merged.provenance_type.isin(['seed','synonym'])]\n",
    "related_df = related_merged_df.loc[~related_merged_df.id.isin(core_df.id)]\n",
    "\n",
    "print(core_df.shape,related_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_senses = {'machine_nn01-38474233','machine_nn01-38474548','machine_nn01-38475164','machine_nn01-38475286',\n",
    "          \"machine_nn01-38474607\",\"machine_nn01-38475923\",\"machine_nn01-38474877\",\"machine_nn01-38475046\",\n",
    "          \"machine_nn01-38475099\"\n",
    "         }\n",
    "\n",
    "def select_senses(df,list_senses):\n",
    "    sc_ids = set([s for l in df.loc[df.id.isin(list_senses)].semantic_class_last_id.to_list() for s in l])\n",
    "    \n",
    "    overlap = lambda x,l: bool(set(x).intersection(l))\n",
    "    \n",
    "    df = df.loc[(df.id.isin(list_senses))  | (df.provenance.isin(list_senses)) | (df.provenance.isin(sc_ids))]\n",
    "    \n",
    "    return df\n",
    "\n",
    "core_selected_df = select_senses(core_df,include_senses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_not_selected_df =  core_df.loc[~core_df.id.isin(core_selected_df.id)]\n",
    "print(core_df.shape,core_selected_df.shape,core_not_selected_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_sample_df = related_df.sample(frac=.1)\n",
    "\n",
    "# subsampling here!! remove later\n",
    "related_sample_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_sample_df['label'] = 'not_machine'\n",
    "core_not_selected_df['label'] = 'not_machine'\n",
    "core_selected_df['label'] = 'machine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import re\n",
    "    \n",
    "def preprocess_sent(sent: str) -> str: # check if setting sent_id to None effects anything?\n",
    "    \"\"\"preprocessing function for formatting raw text before training word2vec\n",
    "    # Credits: Kasra Hosseini and Kaspar Beelen\n",
    "    Arguments:\n",
    "        sent (string): input sentence\n",
    "        sent_id (string): idx of the inpute sentence\n",
    "        tokenized (boolean): if True then return the string as a list of tokens\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    # --- replace .- and . in the middle of the word\n",
    "    sent = re.sub(r'(?<=\\w)(\\.-)(?=\\w)', '-', sent)\n",
    "    sent = re.sub(r'(?<=\\w)(\\.)(?=\\w)', '', sent)\n",
    "    # --- remove accent\n",
    "    sent = unidecode.unidecode(sent)\n",
    "    # --- remove 2 or more .\n",
    "    sent = re.sub(r'[.]{2,}', '.', sent)\n",
    "    # --- add a space before and after a list of punctuations\n",
    "    sent = re.sub(r\"([.,!?:;\\\"\\'])\", r\" \\1 \", sent)\n",
    "    # --- remove everything except:\n",
    "    sent = re.sub(r\"([^a-zA-Z\\-.:;,!?\\d+]+)\", r\" \", sent)\n",
    "    # --- replace numbers with <NUM>\n",
    "    sent = re.sub(r'\\b\\d+\\b', '<NUM>', sent)\n",
    "    sent = re.sub(r'--', '', sent)\n",
    "    # --- normalize white spaces\n",
    "    sent = re.sub(r'\\s+', ' ', sent)\n",
    "    # --- lowercase\n",
    "    sent = sent.lower()\n",
    "\n",
    "    \n",
    "    return sent    \n",
    "\n",
    "def process_for_classification(text_col):\n",
    "    sentence = preprocess_sent(text_col[\"full_text\"])\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "combined_all = pd.concat([related_sample_df[['text','label']],\n",
    "                          core_not_selected_df[['text','label']],\n",
    "                          core_selected_df[['text','label']],\n",
    "                         ])\n",
    "combined_all['processed_text'] = combined_all.text.apply(process_for_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_all.drop(combined_all[combined_all.processed_text==''].index,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_all['partition'] = [np.random.choice(['train','dev','test'], p=[0.6, 0.2, 0.2]) for _ in range(combined_all.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in ['train','dev','test']:\n",
    "    combined_all.loc[combined_all.partition==split][['processed_text','label']].to_csv(f'./data/{split}.csv',sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import CSVClassificationCorpus\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "path = Path(\"./data\")\n",
    "\n",
    "# column format indicating which columns hold the text and label(s)\n",
    "columns = {1: 'text', 2: 'label'}\n",
    "\n",
    "# load corpus containing training, test and dev data and if CSV has a header, you can skip it\n",
    "corpus: Corpus = CSVClassificationCorpus(path,\n",
    "                                         columns,\n",
    "                                         skip_header=True,\n",
    "                                         delimiter='\\t',    # tab-separated files\n",
    "                                            ) \n",
    "    \n",
    "\n",
    "# 2. create the label dictionary\n",
    "label_dict = corpus.make_label_dictionary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.adam import Adam\n",
    "from flair.data import Corpus\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "# 3. initialize transformer document embeddings (many models are available)\n",
    "document_embeddings = TransformerDocumentEmbeddings('distilbert-base-uncased', fine_tune=True)\n",
    "\n",
    "# 4. create the text classifier\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict)\n",
    "\n",
    "# 5. initialize the text classifier trainer with Adam optimizer\n",
    "trainer = ModelTrainer(classifier, corpus, optimizer=Adam)\n",
    "\n",
    "# 6. start the training\n",
    "trainer.train('resources/classifier/machine',\n",
    "              learning_rate=3e-5, # use very small learning rate\n",
    "              mini_batch_size=16,\n",
    "              mini_batch_chunk_size=4, # optionally set this if transformer is too much for your machine\n",
    "              max_epochs=5, # terminate after 5 epochs\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_for_sequence_tagging(text_col):\n",
    "    \n",
    "    punct = [',','.',' ','?','!']\n",
    "    sentence = text_col[\"full_text\"]\n",
    "    \n",
    "    sentence = np.array([i for i in sentence])\n",
    "    offset = text_col[\"keyword_offset\"]\n",
    "    target = text_col[\"keyword\"]\n",
    "    \n",
    "    if not target:\n",
    "        return None\n",
    "    \n",
    "    labels = np.array([0]*len(sentence))\n",
    "    end = offset + len(target)\n",
    "    labels[offset:end] = 1\n",
    "    \n",
    "    for ch in punct:\n",
    "        labels[np.where(sentence==ch)] = 2\n",
    "    \n",
    "    rows = []\n",
    "    word,labs = [],[]\n",
    "    \n",
    "    for i in range(len(sentence)):\n",
    "        if labels[i] < 2:\n",
    "            word.append(sentence[i])\n",
    "            labs.append(labels[i])\n",
    "        \n",
    "        if labels[i] == 2 and word:\n",
    "            rows.append((''.join(word),{0:\"notmachine\",1:\"machine\"}[list(set(labs))[0]]))\n",
    "            word,labs = [],[]\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df2string(df):\n",
    "    return \"\\n\\n\".join(['\\n'.join(['\\t'.join(e) for e in l]) for l in df.tagged.to_list() if l])\n",
    "\n",
    "train,test,dev = df2string(df_train),df2string(df_test),df2string(df_dev)\n",
    "\n",
    "with open('./data/train.csv','w') as out_doc:\n",
    "    out_doc.write(train)\n",
    "with open('./data/test.csv','w') as out_doc:\n",
    "    out_doc.write(test)\n",
    "with open('./data/dev.csv','w') as out_doc:\n",
    "    out_doc.write(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import UD_ENGLISH\n",
    "from flair.embeddings import WordEmbeddings,FlairEmbeddings,StackedEmbeddings,TransformerWordEmbeddings\n",
    "\n",
    "# 3. make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type='label')\n",
    "print(tag_dictionary)\n",
    "\n",
    "\n",
    "# 4. initialize embeddings\n",
    "embedding_types = [\n",
    "\n",
    "    WordEmbeddings('glove'),\n",
    "\n",
    "    # comment in this line to use character embeddings\n",
    "    # CharacterEmbeddings(),\n",
    "\n",
    "    # comment in these lines to use flair embeddings\n",
    "    FlairEmbeddings('news-forward'),\n",
    "    FlairEmbeddings('news-backward'),\n",
    "]\n",
    "\n",
    "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "\n",
    "#embeddings = TransformerWordEmbeddings('bert-base-cased',fine_tune=True, allow_long_sentences=True)\n",
    "    \n",
    "# 5. initialize sequence tagger\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
    "                                        embeddings=embeddings,\n",
    "                                        tag_dictionary=tag_dictionary,\n",
    "                                        tag_type='label',\n",
    "                                        use_crf=True)\n",
    "\n",
    "# 6. initialize trainer\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "# 7. start training\n",
    "trainer.train('resources/taggers/example-pos',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              max_epochs=150)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
