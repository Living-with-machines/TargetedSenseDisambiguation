{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset_download import *\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path, PosixPath\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import API credentials\n",
    "with open('oed_experiments/oed_credentials.json') as f:\n",
    "    credentials = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lemma\n",
    "lemma_id = \"machine_nn01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path(\"./data\")\n",
    "save_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query the API and get the json response\n",
    "sense_json = query_oed(credentials,'word',lemma_id,flags='include_senses=true&include_quotations=true')\n",
    "\n",
    "# convert the json in a dataframe\n",
    "senses_df = convert_json_to_dataframe(sense_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe\n",
    "# as pickle\n",
    "senses_df.to_pickle(save_path / f\"senses_{lemma_id}.pickle\")\n",
    "# as csv\n",
    "senses_df.to_csv(save_path / f\"senses_{lemma_id}.tsv\",sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open pickle file to avoid calling the API again\n",
    "with open(save_path / f\"senses_{lemma_id}.pickle\",'rb') as in_pickle:\n",
    "    machine_senses_df = pickle.load(in_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all senses that are siblings and descendants\n",
    "# of the semantic class of senses listed in previously obtained query \n",
    "responses = traverse_thesaurus(credentials,machine_senses_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traverse tree or load responses \n",
    "# responses = traverse_thesaurus(credentials,machine_senses_df)\n",
    "with open('./data/tree_traversal.pickle','rb') as in_pickle:\n",
    "    responses = pickle.load(in_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all quoations for the senses in the responses variable\n",
    "quotations = get_quotations_from_thesaurus(credentials,responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge and save all information stored in the seperate pickle files\n",
    "df = merge_pickled(Path(\"./data/senses_machine_nn01.pickle\"),\n",
    "                   Path(\"./data/tree_traversal.pickle\"),\n",
    "                   Path(\"./data/tree_traversal_quotations.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(f\"./data/{lemma_id}_all.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1-dataframe\u001b[m\r\n",
      "* \u001b[32m19-machine-tagger\u001b[m\r\n",
      "  3-group-senses\u001b[m\r\n",
      "  4-semantic-provenance\u001b[m\r\n",
      "  dev\u001b[m\r\n",
      "  master\u001b[m\r\n",
      "  oed-experiments\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!git branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start,end = 1750,1950\n",
    "lemma_id = 'machine_nn01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_last_id = lambda nested_list :[l[-1] for l in nested_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_from_lemma_query(auth,lemma_id,start=1750,end=1950):\n",
    "    \"\"\"Extends senses from a dataframe generate from accessing\n",
    "    the API via the word endpoint. The script first retrieves all\n",
    "    senses, then synonyms for these senses, then other senses that \n",
    "    match the semantic classes of the retrieved senses.\n",
    "    \n",
    "    This script also aims to record the \"provenance\" of words, \n",
    "    their relation to the initial query, which can help to \n",
    "    select of filter words later on.\n",
    "    \n",
    "    Arguments:\n",
    "        lemma_id (str)\n",
    "        start (int)\n",
    "        end (int)\n",
    "    Returns\n",
    "        a pandas.DataFrame\n",
    "    \"\"\"\n",
    "    # load seed query dataframe\n",
    "    query_df = pd.read_pickle(f\"./data/senses_{lemma_id}.pickle\")\n",
    "    \n",
    "    # use the sense endpoint to ensure all information \n",
    "    # can be properly concatenated in one dataframe\n",
    "    \n",
    "    # retrieve all sense ids\n",
    "    query_sense_ids = query_df.id.unique()\n",
    "    \n",
    "    # get all senses by sense id\n",
    "    print(f\"Get all sense for the lemma {lemma_id}\")\n",
    "    seeds = [(s,query_oed(auth,'sense',s,\n",
    "                    flags=f\"current_in='{start}-{end}'&limit=1000\"))\n",
    "                        for s in tqdm(query_sense_ids)]\n",
    "    \n",
    "    # convert to dataframe\n",
    "    seeds_df = pd.DataFrame([seed['data'] for s_id,seed in seeds])\n",
    "    \n",
    "    # define provenance, these words are \"seed\"\n",
    "    seeds_df['provenance'] = seeds_df.id\n",
    "    seeds_df['provenance_type'] = 'seed'\n",
    "    \n",
    "    # get all synonyms for the seed senses\n",
    "    print(f\"Get all synonyms of the senses listed in {lemma_id}\")\n",
    "    synonyms = [(s,query_oed(auth,'sense',s,\n",
    "                    level='synonyms',\n",
    "                    flags=f\"current_in='{start}-{end}'&limit=1000\"))\n",
    "                            for s in tqdm(query_sense_ids)]\n",
    "\n",
    "    # transform list of synonyms to a dataframe\n",
    "    synonyms_df = pd.DataFrame([s for s_id,syn in synonyms for s in syn['data']])\n",
    "    \n",
    "    # these items have provenancy type \"synonym\"\n",
    "    synonyms_df['provenance'] = [s_id for s_id,syn in synonyms for s in syn['data']]\n",
    "    synonyms_df['provenance_type'] = 'synonym'\n",
    "    \n",
    "    # seed + synonyms constitute the nucleas of our query\n",
    "    # branch from there\n",
    "    core_df = pd.concat([seeds_df,synonyms_df])\n",
    "    core_df['semantic_class_last_id'] = core_df['semantic_class_ids'].apply(get_last_id)\n",
    "    \n",
    "    # retrieve all semantic class ids for the senses so far\n",
    "    semantic_class_ids = set([s for l in core_df.semantic_class_last_id.to_list() for s in l])\n",
    "\n",
    "    # get all the branches for the retrieve semantic class ids\n",
    "    print(\"Get all branches for seed senses and synonyms\")\n",
    "    branches = [(idx,query_oed(auth,'semanticclass', idx, \n",
    "                        level='branchsenses',\n",
    "                        flags=f\"current_in='{start}-{end}'&limit=1000\"))\n",
    "                            for idx in tqdm(semantic_class_ids)]\n",
    "    \n",
    "    # convert API response to dataframe\n",
    "    branches_df = pd.DataFrame([s for idx,branch in branches for s in branch['data']])\n",
    "    \n",
    "    # provenance_type is branch with semantic class id \n",
    "    # that was use for retrieving the sense is the provenance\n",
    "    branches_df['provenance'] = [idx for idx,branch in branches for s in branch['data']]\n",
    "    branches_df['provenance_type'] = 'branch'\n",
    "    \n",
    "    branches_df['semantic_class_last_id'] = branches_df.semantic_class_ids.apply(get_last_id)\n",
    "    \n",
    "    # remove senses that already appear in the core_df\n",
    "    branches_df_red = branches_df.loc[~branches_df.id.isin(core_df.id)]\n",
    "    \n",
    "    # concatenate core and branch senses\n",
    "    extended_df = pd.concat([core_df,branches_df_red])\n",
    "    \n",
    "    # refine the provenance type\n",
    "    # if the last semantic class id is not equal to provenance\n",
    "    # this row is a child or descendant\n",
    "    check_membership = lambda row : row.provenance in row.semantic_class_last_id\n",
    "    extended_df.loc[(~extended_df.apply(check_membership,axis=1)) & (extended_df.provenance_type=='branch'),\n",
    "                [\"provenance_type\"]]  = \"branch_descendant\"\n",
    "    \n",
    "    # save information\n",
    "    extended_df.to_pickle(f'./data/senses_{lemma_id}_extended.pickle')\n",
    "    return extended_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_df = extend_from_lemma_query(credentials,lemma_id,start,end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"Aaron's rod\",\n",
       " 'Durex',\n",
       " 'French letter',\n",
       " 'Frenchy',\n",
       " \"God's image\",\n",
       " 'Jacuzzi',\n",
       " 'John',\n",
       " 'John Henry',\n",
       " 'John Thomas',\n",
       " 'Johnny',\n",
       " 'Johnson',\n",
       " 'Lizzie',\n",
       " 'M.F.V.',\n",
       " 'Mudian',\n",
       " 'Percy',\n",
       " 'Peter',\n",
       " 'Roger',\n",
       " 'Trojan',\n",
       " 'Turing machine',\n",
       " 'address',\n",
       " 'aerobat',\n",
       " 'aerocar',\n",
       " 'aerostat',\n",
       " 'affair',\n",
       " 'air machine',\n",
       " 'air vessel',\n",
       " 'aircraft',\n",
       " 'anatomy',\n",
       " 'appliance',\n",
       " 'ark',\n",
       " 'armour',\n",
       " 'arrangement',\n",
       " 'arrow',\n",
       " 'art',\n",
       " 'artifice',\n",
       " 'automaton',\n",
       " 'avion',\n",
       " 'baby-maker',\n",
       " 'barge',\n",
       " 'bark',\n",
       " 'bastiment',\n",
       " 'bathing-machine',\n",
       " 'beam',\n",
       " 'being',\n",
       " 'belly',\n",
       " 'bike',\n",
       " 'biocomputer',\n",
       " 'blood bulk',\n",
       " 'board',\n",
       " 'bodiȝlich',\n",
       " 'body',\n",
       " 'bone house',\n",
       " 'bones',\n",
       " 'bottom',\n",
       " 'bouk',\n",
       " 'bubble',\n",
       " 'buggy',\n",
       " 'bulk',\n",
       " 'bus',\n",
       " 'buzz-box',\n",
       " 'buzz-wagon',\n",
       " 'cabinet',\n",
       " 'cade',\n",
       " 'cadre',\n",
       " 'car',\n",
       " 'carcass',\n",
       " 'carriage',\n",
       " 'carrion',\n",
       " 'casa',\n",
       " 'case',\n",
       " 'cast',\n",
       " 'cautel',\n",
       " 'chariot',\n",
       " 'chassis',\n",
       " 'chevisance',\n",
       " 'chode',\n",
       " 'class',\n",
       " 'clay',\n",
       " 'clipper',\n",
       " 'clod',\n",
       " 'cock',\n",
       " 'cod',\n",
       " 'compass',\n",
       " 'compassing',\n",
       " 'compilement',\n",
       " 'composition',\n",
       " 'compound',\n",
       " 'condom',\n",
       " 'conject',\n",
       " 'conjecture',\n",
       " 'contignation',\n",
       " 'contraption',\n",
       " 'contrevure',\n",
       " 'contrival',\n",
       " 'contrivance',\n",
       " 'contrivement',\n",
       " 'convenience',\n",
       " 'conveniency',\n",
       " 'conveyance',\n",
       " 'corporation',\n",
       " 'corporeity',\n",
       " 'corpse',\n",
       " 'corpus',\n",
       " 'corse',\n",
       " 'cost',\n",
       " 'craft',\n",
       " 'cran',\n",
       " 'cunning',\n",
       " 'cust',\n",
       " 'cycle',\n",
       " 'device',\n",
       " 'dial',\n",
       " 'dick',\n",
       " 'dicky',\n",
       " 'dildo',\n",
       " 'diligence',\n",
       " 'dilly',\n",
       " 'ding-a-ling',\n",
       " 'dodge',\n",
       " 'dog',\n",
       " 'dong',\n",
       " 'dork',\n",
       " 'drag',\n",
       " 'drum',\n",
       " 'dust',\n",
       " 'dwelling-house',\n",
       " 'earth',\n",
       " 'easel-picture',\n",
       " 'easel-piece',\n",
       " 'embarkation',\n",
       " 'embodiment',\n",
       " 'energizer',\n",
       " 'engine',\n",
       " 'excogitation',\n",
       " 'expediency',\n",
       " 'expedient',\n",
       " 'expediment',\n",
       " 'fabric',\n",
       " 'fabrication',\n",
       " 'fancy',\n",
       " 'fanglement',\n",
       " 'fashion',\n",
       " 'ficelle',\n",
       " 'fidcock',\n",
       " 'finding',\n",
       " 'fire engine',\n",
       " 'firehouse',\n",
       " 'flesh',\n",
       " 'flesh-rind',\n",
       " 'flesh-stuff',\n",
       " 'flood-bickerer',\n",
       " 'flute',\n",
       " 'fly',\n",
       " 'flyer',\n",
       " 'flying post',\n",
       " 'forcing-engine',\n",
       " 'frame',\n",
       " 'framework',\n",
       " 'gadget',\n",
       " 'generator',\n",
       " 'gimcrack',\n",
       " 'gin',\n",
       " 'golem',\n",
       " 'graith',\n",
       " 'gun',\n",
       " 'handle',\n",
       " 'high-flyer',\n",
       " 'home',\n",
       " 'hooker',\n",
       " 'horse',\n",
       " 'house',\n",
       " 'housing',\n",
       " 'implements',\n",
       " 'incarnation',\n",
       " 'industry',\n",
       " 'ingenuity',\n",
       " 'ingeny',\n",
       " 'instrument',\n",
       " 'intromittent',\n",
       " 'invent',\n",
       " 'invention',\n",
       " 'jam-jar',\n",
       " 'jet',\n",
       " 'jigamaree',\n",
       " 'jock',\n",
       " 'kakemono',\n",
       " 'keel',\n",
       " 'khazi',\n",
       " 'kink',\n",
       " 'kipsie',\n",
       " 'knack',\n",
       " 'knob',\n",
       " 'langer',\n",
       " 'lay',\n",
       " 'length',\n",
       " 'letter',\n",
       " 'lich',\n",
       " 'licham',\n",
       " 'lock',\n",
       " 'longhouse',\n",
       " 'loom',\n",
       " 'low rider',\n",
       " 'lump of clay',\n",
       " 'lurk',\n",
       " 'machina',\n",
       " 'machinament',\n",
       " 'machination',\n",
       " 'machine for living',\n",
       " 'machine power',\n",
       " 'machinery',\n",
       " 'mail coach',\n",
       " 'mail packet',\n",
       " 'mail stage',\n",
       " 'mail-hack',\n",
       " 'maison',\n",
       " 'man',\n",
       " 'man-machine',\n",
       " 'management',\n",
       " 'manhood',\n",
       " 'manoeuvre',\n",
       " 'mansion house',\n",
       " 'meat tool',\n",
       " 'mechanical advantage',\n",
       " 'mechanism',\n",
       " 'membrum virile',\n",
       " 'mentula',\n",
       " 'mickey',\n",
       " 'micky',\n",
       " 'microcosm',\n",
       " 'micrograph',\n",
       " 'middle leg',\n",
       " 'mister',\n",
       " 'mole',\n",
       " 'molition',\n",
       " 'monkey',\n",
       " 'motor',\n",
       " 'motor car',\n",
       " 'motor carriage',\n",
       " 'move',\n",
       " 'mover',\n",
       " 'mud wall',\n",
       " 'nag',\n",
       " 'nanny',\n",
       " 'nanny-goat',\n",
       " 'needle',\n",
       " 'nerve',\n",
       " 'noonies',\n",
       " 'nymph',\n",
       " 'old fellow',\n",
       " 'old man',\n",
       " 'organ',\n",
       " 'other thing',\n",
       " 'oustil',\n",
       " 'outwall',\n",
       " 'outward man',\n",
       " 'pageant',\n",
       " 'passage',\n",
       " 'pata',\n",
       " 'pecker',\n",
       " 'pee-pee',\n",
       " 'pego',\n",
       " 'pencil',\n",
       " 'penis',\n",
       " 'person',\n",
       " 'personage',\n",
       " 'personality',\n",
       " 'pillicock',\n",
       " 'pillock',\n",
       " 'pine',\n",
       " 'pintle',\n",
       " 'pisser',\n",
       " 'pitchboard',\n",
       " 'pizzle',\n",
       " 'plant',\n",
       " 'ploy',\n",
       " 'poker',\n",
       " 'police',\n",
       " 'policy',\n",
       " 'pork',\n",
       " 'port',\n",
       " 'post calash',\n",
       " 'post vehicle',\n",
       " 'post-car',\n",
       " 'post-carriage',\n",
       " 'post-coach',\n",
       " 'post-equipage',\n",
       " 'post-stage',\n",
       " 'post-wagon',\n",
       " 'power',\n",
       " 'prependent',\n",
       " 'prick',\n",
       " 'prime mover',\n",
       " 'primum mobile',\n",
       " 'privy',\n",
       " 'prong',\n",
       " 'propagator',\n",
       " 'prophylactic',\n",
       " 'prore',\n",
       " 'protective',\n",
       " 'prow',\n",
       " 'pud',\n",
       " 'pudding',\n",
       " 'putz',\n",
       " 'quadrant',\n",
       " 'quaint',\n",
       " 'quaintise',\n",
       " 'quarrons',\n",
       " 'quiff',\n",
       " 'quillity',\n",
       " 'rammer',\n",
       " 'recipe',\n",
       " 'ride',\n",
       " 'rig',\n",
       " 'robot',\n",
       " 'roof',\n",
       " 'root',\n",
       " 'rubber',\n",
       " 'rubigo',\n",
       " 'ruler',\n",
       " 'runnion',\n",
       " 'safe',\n",
       " 'safety',\n",
       " 'schlong',\n",
       " 'scoot',\n",
       " 'scooter',\n",
       " 'scroll painting',\n",
       " 'scroll picture',\n",
       " 'scumbag',\n",
       " 'sensuality',\n",
       " 'sex',\n",
       " 'shack',\n",
       " 'shaft',\n",
       " 'sheath',\n",
       " 'shebang',\n",
       " 'shift',\n",
       " 'ship',\n",
       " 'short',\n",
       " 'skift',\n",
       " 'skinful',\n",
       " 'slime',\n",
       " 'smoke-house',\n",
       " 'soma',\n",
       " 'soul case',\n",
       " 'spell',\n",
       " 'stage',\n",
       " 'stage-wagon',\n",
       " 'stagecoach',\n",
       " 'stager',\n",
       " 'standing house',\n",
       " 'steer',\n",
       " 'stern-bearer',\n",
       " 'stick',\n",
       " 'stroke',\n",
       " 'structure',\n",
       " 'subtility',\n",
       " 'sulky',\n",
       " 'swill',\n",
       " 'swipe',\n",
       " 'tabernacle',\n",
       " 'tadger',\n",
       " 'tally-ho',\n",
       " 'tarse',\n",
       " 'tondo',\n",
       " 'tonk',\n",
       " 'tool',\n",
       " 'totalizator',\n",
       " 'totalizer',\n",
       " 'totalizing',\n",
       " 'tote',\n",
       " 'tour',\n",
       " 'traction',\n",
       " 'trade',\n",
       " 'tram',\n",
       " 'transportation',\n",
       " 'tree',\n",
       " 'trick',\n",
       " 'triumphal',\n",
       " 'trunk',\n",
       " 'tub',\n",
       " 'tube',\n",
       " 'utensil',\n",
       " 'utter man',\n",
       " 'vehicle',\n",
       " 'vehiculum',\n",
       " 'verge',\n",
       " 'vessel',\n",
       " 'vice',\n",
       " 'virge',\n",
       " 'virile member',\n",
       " 'virilia',\n",
       " 'voiture',\n",
       " 'wagon',\n",
       " 'wagon-coach',\n",
       " 'wanker',\n",
       " 'water engine',\n",
       " 'water house',\n",
       " 'water treader',\n",
       " 'watercraft',\n",
       " 'weapon',\n",
       " 'wee-wee',\n",
       " 'whang',\n",
       " 'whanger',\n",
       " 'wheel',\n",
       " 'wheels',\n",
       " 'whore-pipe',\n",
       " 'wile',\n",
       " 'willy',\n",
       " 'winkle',\n",
       " 'winky',\n",
       " 'wooden horse',\n",
       " 'wooden isle',\n",
       " 'work-loom',\n",
       " 'wrinkle',\n",
       " 'yard'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(merged[(merged.provenance_type=='synonym') & (merged.lemma!='machine')].lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quotations(lemma_id,sense_df=''):\n",
    "    \"\"\"Obtain and store all quotations from a dataframe constructed\n",
    "    from information retrieved via de sense endpoint.\n",
    "    \n",
    "    Arguments:\n",
    "        sense_df (pandas.DataFrame,str)\n",
    "        lemma_id (str)\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame\n",
    "    \"\"\"\n",
    "    if isinstance(sense_df,str):\n",
    "        quotations_df = pd.read_pickle(f'./data/quotations_{lemma_id}.pickle')\n",
    "    else:\n",
    "        quotations = [query_oed(credentials,'sense',sense_idx,level='quotations')\n",
    "                        for sense_idx in tqdm(set(sense_df.id))]\n",
    "            \n",
    "        quotations_df = pd.concat([pd.DataFrame(q['data']) for q in quotations])\n",
    "        quotations_df.to_pickle(f'./data/quotations_{lemma_id}.pickle')\n",
    "    return quotations_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotations_df = get_quotations(lemma_id)\n",
    "quotations_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = extended_df.merge(quotations_df[['id','sense_id',\"text\",\"year\",\"source\"]],left_on='id',right_on='sense_id',suffixes=['',\"_quotation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging experiment with seed and synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_forms = set(extended_df[extended_df.provenance_type.isin(['seed','synonym'])].lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma_content_from_surface_form(surface_forms):\n",
    "    \"\"\"Get all quotations and senses from a list of surface forms\n",
    "    \"\"\"\n",
    "    lemmatized = [query_oed(credentials,'lemmatize','',flags=f\"form={sf}\")\n",
    "                             for sf in tqdm(surface_forms)]\n",
    "    \n",
    "    lemmas_content = [query_oed(credentials, 'word',\n",
    "                       l['word']['id'], \n",
    "                       flags=\"include_senses=true&include_quotations=true\") \n",
    "                             for lemma in tqdm(lemmatized)\n",
    "                                 for l in lemma['data']]\n",
    "    \n",
    "    lemmas_content_df = pd.DataFrame(sense for lemma in lemmas_content for sense in lemma['data']['senses'])\n",
    "    \n",
    "    quotations = [quotation\n",
    "                        for i,row in lemmas_content_df.iterrows()\n",
    "                             for quotation in row.quotations]\n",
    "    \n",
    "    related_quotations_df = pd.DataFrame(quotations)\n",
    "    \n",
    "    related_merged_df = lemmas_content_df.merge(related_quotations_df[['id','sense_id',\"text\",\"year\",\"source\"]],left_on='id',right_on='sense_id',suffixes=['',\"_quotation\"])\n",
    "    \n",
    "    related_merged_df['provenance'] = None\n",
    "    related_merged_df['provenance_type'] = 'related'\n",
    "    #related_merged_df.loc[related_merged_df.id.isin(merged_df.id),['provenance_type']] = 'in_core'\n",
    "\n",
    "    return related_merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start here if data is downloaded and put into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_merged_df = pd.read_pickle(f'./data/related_merged_{lemma_id}.pickle')\n",
    "merged = pd.read_pickle(f'./data/merged_{lemma_id}.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#related_merged_df.to_pickle(f'./data/related_merged_{lemma_id}.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2975, 24) (86977, 23)\n"
     ]
    }
   ],
   "source": [
    "core_df = merged[merged.provenance_type.isin(['seed','synonym'])]\n",
    "related_df = related_merged_df.loc[~related_merged_df.id.isin(core_df.id)]\n",
    "\n",
    "print(core_df.shape,related_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_senses = {'machine_nn01-38474233','machine_nn01-38474548','machine_nn01-38475164','machine_nn01-38475286',\n",
    "          \"machine_nn01-38474607\",\"machine_nn01-38475923\",\"machine_nn01-38474877\",\"machine_nn01-38475046\",\n",
    "          \"machine_nn01-38475099\"\n",
    "         }\n",
    "\n",
    "def select_senses(df,list_senses):\n",
    "    sc_ids = set([s for l in df.loc[df.id.isin(list_senses)].semantic_class_last_id.to_list() for s in l])\n",
    "    \n",
    "    overlap = lambda x,l: bool(set(x).intersection(l))\n",
    "    \n",
    "    df = df.loc[(df.id.isin(list_senses))  | (df.provenance.isin(list_senses)) | (df.provenance.isin(sc_ids))]\n",
    "    \n",
    "    return df\n",
    "\n",
    "core_selected_df = select_senses(core_df,include_senses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Lizzie',\n",
       " 'M.F.V.',\n",
       " 'Mudian',\n",
       " 'address',\n",
       " 'aerobat',\n",
       " 'aerocar',\n",
       " 'aerostat',\n",
       " 'affair',\n",
       " 'air machine',\n",
       " 'air vessel',\n",
       " 'aircraft',\n",
       " 'appliance',\n",
       " 'ark',\n",
       " 'art',\n",
       " 'avion',\n",
       " 'barge',\n",
       " 'bark',\n",
       " 'bastiment',\n",
       " 'beam',\n",
       " 'board',\n",
       " 'bottom',\n",
       " 'bubble',\n",
       " 'buggy',\n",
       " 'bus',\n",
       " 'buzz-box',\n",
       " 'buzz-wagon',\n",
       " 'car',\n",
       " 'carriage',\n",
       " 'cautel',\n",
       " 'chariot',\n",
       " 'class',\n",
       " 'clipper',\n",
       " 'compassing',\n",
       " 'contraption',\n",
       " 'contrivance',\n",
       " 'convenience',\n",
       " 'conveniency',\n",
       " 'conveyance',\n",
       " 'craft',\n",
       " 'cycle',\n",
       " 'device',\n",
       " 'dial',\n",
       " 'diligence',\n",
       " 'dilly',\n",
       " 'dog',\n",
       " 'drag',\n",
       " 'embarkation',\n",
       " 'energizer',\n",
       " 'engine',\n",
       " 'fabric',\n",
       " 'fancy',\n",
       " 'fanglement',\n",
       " 'fashion',\n",
       " 'flood-bickerer',\n",
       " 'fly',\n",
       " 'flyer',\n",
       " 'flying post',\n",
       " 'frame',\n",
       " 'generator',\n",
       " 'gimcrack',\n",
       " 'gin',\n",
       " 'graith',\n",
       " 'gun',\n",
       " 'high-flyer',\n",
       " 'hooker',\n",
       " 'horse',\n",
       " 'implements',\n",
       " 'instrument',\n",
       " 'invention',\n",
       " 'jam-jar',\n",
       " 'jet',\n",
       " 'keel',\n",
       " 'loom',\n",
       " 'low rider',\n",
       " 'machina',\n",
       " 'machinament',\n",
       " 'machination',\n",
       " 'machine',\n",
       " 'machine power',\n",
       " 'mail coach',\n",
       " 'mail packet',\n",
       " 'mail stage',\n",
       " 'mail-hack',\n",
       " 'man',\n",
       " 'mechanical advantage',\n",
       " 'mechanism',\n",
       " 'mister',\n",
       " 'molition',\n",
       " 'motor',\n",
       " 'motor car',\n",
       " 'motor carriage',\n",
       " 'mover',\n",
       " 'nymph',\n",
       " 'oustil',\n",
       " 'pageant',\n",
       " 'passage',\n",
       " 'pine',\n",
       " 'pitchboard',\n",
       " 'plant',\n",
       " 'port',\n",
       " 'post calash',\n",
       " 'post vehicle',\n",
       " 'post-car',\n",
       " 'post-carriage',\n",
       " 'post-coach',\n",
       " 'post-equipage',\n",
       " 'post-stage',\n",
       " 'post-wagon',\n",
       " 'power',\n",
       " 'prime mover',\n",
       " 'primum mobile',\n",
       " 'prore',\n",
       " 'prow',\n",
       " 'quaint',\n",
       " 'ride',\n",
       " 'rig',\n",
       " 'ruler',\n",
       " 'scoot',\n",
       " 'scooter',\n",
       " 'shebang',\n",
       " 'ship',\n",
       " 'short',\n",
       " 'stage',\n",
       " 'stage-wagon',\n",
       " 'stagecoach',\n",
       " 'stager',\n",
       " 'steer',\n",
       " 'stern-bearer',\n",
       " 'tally-ho',\n",
       " 'tool',\n",
       " 'traction',\n",
       " 'tram',\n",
       " 'transportation',\n",
       " 'tree',\n",
       " 'trick',\n",
       " 'triumphal',\n",
       " 'utensil',\n",
       " 'vehicle',\n",
       " 'vehiculum',\n",
       " 'vessel',\n",
       " 'vice',\n",
       " 'voiture',\n",
       " 'wagon',\n",
       " 'wagon-coach',\n",
       " 'water house',\n",
       " 'water treader',\n",
       " 'watercraft',\n",
       " 'wheel',\n",
       " 'wheels',\n",
       " 'wooden horse',\n",
       " 'wooden isle',\n",
       " 'work-loom'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(core_selected_df.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2975, 24) (1113, 24) (1833, 24)\n"
     ]
    }
   ],
   "source": [
    "core_not_selected_df =  core_df.loc[~core_df.id.isin(core_selected_df.id)]\n",
    "print(core_df.shape,core_selected_df.shape,core_not_selected_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8698, 23)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_sample_df = related_df.sample(frac=.1)\n",
    "\n",
    "# subsampling here!! remove later\n",
    "related_sample_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "related_sample_df['label'] = 'not_machine'\n",
    "core_not_selected_df['label'] = 'not_machine'\n",
    "core_selected_df['label'] = 'machine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import re\n",
    "    \n",
    "def preprocess_sent(sent: str) -> str: # check if setting sent_id to None effects anything?\n",
    "    \"\"\"preprocessing function for formatting raw text before training word2vec\n",
    "    # Credits: Kasra Hosseini and Kaspar Beelen\n",
    "    Arguments:\n",
    "        sent (string): input sentence\n",
    "        sent_id (string): idx of the inpute sentence\n",
    "        tokenized (boolean): if True then return the string as a list of tokens\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    # --- replace .- and . in the middle of the word\n",
    "    sent = re.sub(r'(?<=\\w)(\\.-)(?=\\w)', '-', sent)\n",
    "    sent = re.sub(r'(?<=\\w)(\\.)(?=\\w)', '', sent)\n",
    "    # --- remove accent\n",
    "    sent = unidecode.unidecode(sent)\n",
    "    # --- remove 2 or more .\n",
    "    sent = re.sub(r'[.]{2,}', '.', sent)\n",
    "    # --- add a space before and after a list of punctuations\n",
    "    sent = re.sub(r\"([.,!?:;\\\"\\'])\", r\" \\1 \", sent)\n",
    "    # --- remove everything except:\n",
    "    sent = re.sub(r\"([^a-zA-Z\\-.:;,!?\\d+]+)\", r\" \", sent)\n",
    "    # --- replace numbers with <NUM>\n",
    "    sent = re.sub(r'\\b\\d+\\b', '<NUM>', sent)\n",
    "    sent = re.sub(r'--', '', sent)\n",
    "    # --- normalize white spaces\n",
    "    sent = re.sub(r'\\s+', ' ', sent)\n",
    "    # --- lowercase\n",
    "    sent = sent.lower()\n",
    "\n",
    "    \n",
    "    return sent    \n",
    "\n",
    "def process_for_classification(text_col):\n",
    "    sentence = preprocess_sent(text_col[\"full_text\"])\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "combined_all = pd.concat([related_sample_df[['text','label']],\n",
    "                          core_not_selected_df[['text','label']],\n",
    "                          core_selected_df[['text','label']],\n",
    "                         ])\n",
    "combined_all['processed_text'] = combined_all.text.apply(process_for_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_all.drop(combined_all[combined_all.processed_text==''].index,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11598, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'keyword': 'machins',\n",
       " 'full_text': 'For all that, their lucke was at that time, to loose man, moyle, and machins belonging to warre.',\n",
       " 'keyword_offset': 69}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_all[combined_all.label=='machine'].iloc[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_all['partition'] = [np.random.choice(['train','dev','test'], p=[0.6, 0.2, 0.2]) for _ in range(combined_all.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in ['train','dev','test']:\n",
    "    combined_all.loc[combined_all.partition==split][['processed_text','label']].to_csv(f'./data/{split}.csv',sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import CSVClassificationCorpus\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "path = Path(\"./data\")\n",
    "\n",
    "# column format indicating which columns hold the text and label(s)\n",
    "columns = {1: 'text', 2: 'label'}\n",
    "\n",
    "# load corpus containing training, test and dev data and if CSV has a header, you can skip it\n",
    "corpus: Corpus = CSVClassificationCorpus(path,\n",
    "                                         columns,\n",
    "                                         skip_header=True,\n",
    "                                         delimiter='\\t',    # tab-separated files\n",
    "                                            ) \n",
    "    \n",
    "\n",
    "# 2. create the label dictionary\n",
    "label_dict = corpus.make_label_dictionary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.adam import Adam\n",
    "from flair.data import Corpus\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "# 3. initialize transformer document embeddings (many models are available)\n",
    "document_embeddings = TransformerDocumentEmbeddings('distilbert-base-uncased', fine_tune=True)\n",
    "\n",
    "# 4. create the text classifier\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict)\n",
    "\n",
    "# 5. initialize the text classifier trainer with Adam optimizer\n",
    "trainer = ModelTrainer(classifier, corpus, optimizer=Adam)\n",
    "\n",
    "# 6. start the training\n",
    "trainer.train('resources/classifier/machine',\n",
    "              learning_rate=3e-5, # use very small learning rate\n",
    "              mini_batch_size=16,\n",
    "              mini_batch_chunk_size=4, # optionally set this if transformer is too much for your machine\n",
    "              max_epochs=5, # terminate after 5 epochs\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_for_sequence_tagging(text_col):\n",
    "    \n",
    "    # try spaCy\n",
    "    \n",
    "    punct = [',','.',' ','?','!']\n",
    "    sentence = text_col[\"full_text\"]\n",
    "    \n",
    "    sentence = np.array([i for i in sentence])\n",
    "    offset = text_col[\"keyword_offset\"]\n",
    "    target = text_col[\"keyword\"]\n",
    "    \n",
    "    if not target:\n",
    "        return None\n",
    "    \n",
    "    labels = np.array([0]*len(sentence))\n",
    "    end = offset + len(target)\n",
    "    labels[offset:end] = 1\n",
    "    \n",
    "    for ch in punct:\n",
    "        labels[np.where(sentence==ch)] = 2\n",
    "    \n",
    "    rows = []\n",
    "    word,labs = [],[]\n",
    "    \n",
    "    for i in range(len(sentence)):\n",
    "        if labels[i] < 2:\n",
    "            word.append(sentence[i])\n",
    "            labs.append(labels[i])\n",
    "        \n",
    "        if labels[i] == 2 and word:\n",
    "            rows.append((''.join(word),{0:\"notmachine\",1:\"machine\"}[list(set(labs))[0]]))\n",
    "            word,labs = [],[]\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df2string(df):\n",
    "    return \"\\n\\n\".join(['\\n'.join(['\\t'.join(e) for e in l]) for l in df.tagged.to_list() if l])\n",
    "\n",
    "train,test,dev = df2string(df_train),df2string(df_test),df2string(df_dev)\n",
    "\n",
    "with open('./data/train.csv','w') as out_doc:\n",
    "    out_doc.write(train)\n",
    "with open('./data/test.csv','w') as out_doc:\n",
    "    out_doc.write(test)\n",
    "with open('./data/dev.csv','w') as out_doc:\n",
    "    out_doc.write(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import UD_ENGLISH\n",
    "from flair.embeddings import WordEmbeddings,FlairEmbeddings,StackedEmbeddings,TransformerWordEmbeddings\n",
    "\n",
    "# 3. make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type='label')\n",
    "print(tag_dictionary)\n",
    "\n",
    "\n",
    "# 4. initialize embeddings\n",
    "embedding_types = [\n",
    "\n",
    "    WordEmbeddings('glove'),\n",
    "\n",
    "    # comment in this line to use character embeddings\n",
    "    # CharacterEmbeddings(),\n",
    "\n",
    "    # comment in these lines to use flair embeddings\n",
    "    FlairEmbeddings('news-forward'),\n",
    "    FlairEmbeddings('news-backward'),\n",
    "]\n",
    "\n",
    "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "\n",
    "#embeddings = TransformerWordEmbeddings('bert-base-cased',fine_tune=True, allow_long_sentences=True)\n",
    "    \n",
    "# 5. initialize sequence tagger\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
    "                                        embeddings=embeddings,\n",
    "                                        tag_dictionary=tag_dictionary,\n",
    "                                        tag_type='label',\n",
    "                                        use_crf=True)\n",
    "\n",
    "# 6. initialize trainer\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "# 7. start training\n",
    "trainer.train('resources/taggers/example-pos',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              max_epochs=150)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
