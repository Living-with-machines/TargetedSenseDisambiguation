{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review notebook: harvest senses with provenance\n",
    "\n",
    "Notebook for reviewing functions\n",
    "\n",
    "- `get_provenance_by_semantic_class`\n",
    "- `extend_from_saved_lemma_query`\n",
    "\n",
    "all saved in `utils.dataset_download`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions assume:\n",
    "    - a pickled dataframe with information harvested from the OED word endpoint for a given lemma id\n",
    "\n",
    "What these functions should do:\n",
    "    - for a given lemma id (e.g. `machine_nn01` saved in pickled data)\n",
    "    - get all senses\n",
    "    - for each of the senses get synonyms\n",
    "    - for each of the senses + synonyms, get all branches (siblings and descedants\n",
    "    - keep track of the relation between the initial lemma and sense harvested (this is saved in provenance and provenance_type column\n",
    "    - for more documentation please refer to the code and this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset_download import *\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path, PosixPath\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load credentials, set paths and arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import API credentials\n",
    "with open('oed_experiments/oed_credentials.json') as f:\n",
    "    auth = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lemma\n",
    "lemma_id = \"machine_nn01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path(dp)\n",
    "save_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start,end = 1750,1950\n",
    "lemma_id = 'machine_nn01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_df = extend_from_saved_lemma_query(auth,lemma_id,start,end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_provenance_by_semantic_class(row):\n",
    "    \"\"\"\n",
    "    decide on the relation between the sense and the target querry\n",
    "    here we use the lowest semantic class id to decide on the relation\n",
    "    \n",
    "    if last semantic class id (sc_ids[-1]) == provenance id: then sense is sibling of provenance id\n",
    "    elif provenance semantic class id in the list of semantic class last ids\n",
    "    (but provenance not the last one): then sense is descendant of provenance id\n",
    "    Argument:\n",
    "        row (pd.Series): row of dataframe obtained from branchsenses endpoint\n",
    "    \n",
    "    Returns:\n",
    "        nested listed in the format of [lowest semantic class id, relation, provenance semantic class id]\n",
    "            in other words it said that for a given sense (which can have multiple semantic class ids)\n",
    "            the lowest semantic class id stands in the relation \"sibling\" or \"descendant\" of the \n",
    "            provenance semantic class id\n",
    "    \"\"\"\n",
    "    \n",
    "    provenance = []\n",
    "    \n",
    "    for sc_ids in row.semantic_class_ids:\n",
    "        relation = ''\n",
    "        \n",
    "        # scenario 1\n",
    "        if sc_ids[-1] == row.provenance_pivot:\n",
    "            relation = 'sibling'\n",
    "        \n",
    "        # scenario 2\n",
    "        elif (row.provenance_pivot in sc_ids):\n",
    "            relation = 'descendant'\n",
    "        \n",
    "        # exclude other relation\n",
    "        if relation:\n",
    "            provenance.append([sc_ids[-1], relation, row.provenance_pivot])\n",
    "    \n",
    "    if not provenance:\n",
    "        print(f'No descendants or siblings found for {row.id}')\n",
    " \n",
    "    return provenance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to get last element in a nested list\n",
    "get_last_id = lambda nested_list :[l[-1] for l in nested_list]\n",
    "    \n",
    "# load seed query dataframe\n",
    "query_df = pd.read_pickle(f\"./data/senses_{lemma_id}.pickle\")\n",
    "    \n",
    "# use the sense endpoint to ensure all information \n",
    "# can be properly concatenated in one dataframe\n",
    "    \n",
    "# retrieve all sense ids\n",
    "query_sense_ids = query_df.id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all senses by sense id\n",
    "print(f\"Get all sense for the lemma {lemma_id}\")\n",
    "seeds = [(s,query_oed(auth,'sense',s,\n",
    "                flags=f\"current_in='{start}-{end}'&limit=1000\", # probably \"current_in\" not needed here see APi\n",
    "                verbose=False)) # set verbose to True to see the url request\n",
    "                    for s in tqdm(query_sense_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dataframe\n",
    "seeds_df = pd.DataFrame([seed['data'] for s_id,seed in seeds])\n",
    "\n",
    "# seed_df contains all the senses of the word machine_nn01\n",
    "# we distinguish between provenance and provenance_type\n",
    "# provenance will refer to specific word, sense of semantic class ids\n",
    "# provenance_type will distinguish between different types of extension\n",
    "# define provenance, these words are \"seed\"\n",
    "seeds_df['provenance'] = [[[i,'seed',lemma_id]] for i in seeds_df.id] # for the seed sense we use the id of the word machine_nn0\n",
    "                                       # we use list here, reason is explained later, see provenance of synonyms\n",
    "seeds_df['provenance_type'] = 'seed' # categorize these lemmas as seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all synonyms for the seed senses\n",
    "# reminder synonyms uses same function as the /senses/ endpoint, flags should work here\n",
    "print(f\"Get all synonyms of the senses listed in {lemma_id}\")\n",
    "synonyms = [(s,query_oed(auth,'sense',s,\n",
    "                level='synonyms',\n",
    "                flags=f\"current_in='{start}-{end}'&limit=1000\"))\n",
    "                        for s in tqdm(query_sense_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform list of synonyms to a dataframe\n",
    "synonyms_df = pd.DataFrame([s for s_id,syn in synonyms for s in syn['data']])\n",
    "    \n",
    "# for synonyms the provenance_type is set to \"synonym\"\n",
    "synonyms_df['provenance_type'] = 'synonym'\n",
    "# for synonyms we refer the sense_id via which this synonym was retrieved\n",
    "synonyms_df['provenance'] = [[[s['id'],'synonym',s_id]] for s_id,syn in synonyms for s in syn['data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed + synonyms constitute the nucleas of our query\n",
    "# these are saved in the core_df\n",
    "# shape should be 485 (synonyms senses) + 26 (seed senses)\n",
    "core_df = pd.concat([seeds_df,synonyms_df],sort=True)\n",
    "    \n",
    "# branch out from there\n",
    "# we save the lowest level of the semantic_class_last_id columns\n",
    "core_df['semantic_class_last_id'] = core_df['semantic_class_ids'].apply(get_last_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve all the _lowest_ (or last) semantic class ids for the core senses so far\n",
    "semantic_class_ids = set([s for l in core_df.semantic_class_last_id.to_list() for s in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, we use the descendants endpoint\n",
    "# for each lowest semantic class id\n",
    "# we get all \"descendants\" which according the API documentation\n",
    "# returns an array of senses that belong to the semantic class\n",
    "# specified by ID, plus senses that belong to its child and descendant classes.\n",
    "print(\"Get all branches for seed senses and synonyms\")\n",
    "branches = [(idx,query_oed(auth,'semanticclass', idx, \n",
    "                        level='branchsenses', # \n",
    "                        flags=f\"current_in='{start}-{end}'&limit=1000\"))\n",
    "                            for idx in tqdm(semantic_class_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert API response to dataframe\n",
    "branches_df = pd.DataFrame([s for idx,branch in branches for s in branch['data']])\n",
    "    \n",
    "# ISSUE: again we have duplicate \n",
    "# senses here, as some appear multiple time as\n",
    "# in the same semantic class (or as descendant)\n",
    "    \n",
    "# provenance_type is branch with semantic class id \n",
    "# that was use for retrieving the sense is the provenance\n",
    "branches_df['provenance_type'] = 'branch'\n",
    "    \n",
    "# we create a provenance_pivot columsn, which shows\n",
    "# the semantic class id via which the sense was retrieved\n",
    "branches_df['provenance_pivot'] = [idx for idx, branch in branches for s in branch['data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now there are two scenarios to specify for the pro\n",
    "# both scenarios can apply to one sense\n",
    "# if last semantic class id (sc_ids[-1]) == provenance id: then sense is sibling of provenance id\n",
    "# elif provenance semantic class id in the list of semantic class last ids\n",
    "# (but provenance not the last one): then sense is descendant of provenance id\n",
    "    \n",
    "branches_df['provenance'] = branches_df.apply(get_provenance_by_semantic_class,axis=1)\n",
    "    \n",
    "# drop the provenance_pivot column\n",
    "branches_df.drop('provenance_pivot',axis=1,inplace=True)\n",
    "    \n",
    "# concatenate core and branch senses\n",
    "# ISSUE: have a closer look at the warning message\n",
    "extended_df = pd.concat([core_df,branches_df],sort=True)\n",
    "\n",
    "# to check if rows match\n",
    "#extended_df.shape[0] == core_df.shape[0] + branches_df.shape[0]\n",
    "# save dataframe as pickle\n",
    "extended_df.to_pickle(f\"./data/extended_{lemma_id}.pickle\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
