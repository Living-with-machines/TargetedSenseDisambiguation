{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of results\n",
    "\n",
    "**[WARNING]** To run this notebook you will need to have generated the `lemma` and `quotation` dataframes for each headword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import defaultdict,Counter\n",
    "import statistics\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmas grouped by similarity of their most prominent senses:\n",
    "dLemmas = {\"all_lemmas\": [\"anger_NN\",\"apple_NN\",\"art_NN\", \"democracy_NN\",\n",
    "                  \"happiness_NN\", \"labour_NN\", \"machine_NN\", \"man_NN\",\n",
    "                  \"nation_NN\", \"power_NN\", \"slave_NN\", 'woman_NN'],\n",
    "           \n",
    "           \"non_industrial\": [\"anger_NN\",\"apple_NN\",\"democracy_NN\",\n",
    "                  \"happiness_NN\",\"man_NN\",\n",
    "                  \"nation_NN\",\"slave_NN\",'woman_NN'],\n",
    "           \n",
    "           \"industrial\": [\"art_NN\", \"labour_NN\", \"machine_NN\", \"power_NN\"],\n",
    "           \n",
    "           \"non_tech\": [\"anger_NN\",\"apple_NN\",\"art_NN\", \"democracy_NN\",\n",
    "                  \"happiness_NN\", \"labour_NN\", \"man_NN\",\n",
    "                  \"nation_NN\", \"slave_NN\", 'woman_NN'],\n",
    "           \n",
    "           \"tech\": [\"machine_NN\", \"power_NN\"],\n",
    "           \n",
    "           \"human\": [\"slave_NN\", 'woman_NN', \"man_NN\"],\n",
    "           \n",
    "           \"emotion\": [\"happiness_NN\", \"anger_NN\"],\n",
    "           \n",
    "           \"abstract\": [\"happiness_NN\", \"anger_NN\", \"art_NN\", \"democracy_NN\",\n",
    "                  \"labour_NN\", \"nation_NN\"],\n",
    "           \n",
    "           \"abstract_wo_emotions\": [\"art_NN\", \"democracy_NN\",\n",
    "                  \"labour_NN\", \"nation_NN\"],\n",
    "           \n",
    "           \"concrete\": [\"apple_NN\", \"machine_NN\", \"man_NN\", \"slave_NN\", 'woman_NN'],\n",
    "           \n",
    "           \"concrete_wo_machine\": [\"apple_NN\", \"man_NN\", \"slave_NN\", 'woman_NN'],\n",
    "           \n",
    "           \"man_apple_woman\": [\"apple_NN\", \"man_NN\", 'woman_NN'],\n",
    "           \n",
    "           \"apple\": [\"apple_NN\"],\n",
    "           \n",
    "           \"machine\": [\"machine_NN\"],\n",
    "           \n",
    "           \"slave\": [\"slave_NN\"],\n",
    "           \n",
    "           \"man_woman\": [\"man_NN\", \"woman_NN\"],\n",
    "           \n",
    "           \"all_wo_machine\": [\"anger_NN\",\"apple_NN\",\"art_NN\", \"democracy_NN\",\n",
    "                  \"happiness_NN\", \"labour_NN\", \"man_NN\",\n",
    "                  \"nation_NN\", \"power_NN\", \"slave_NN\", 'woman_NN'],\n",
    "           \n",
    "           \"work_related\": [\"slave_NN\", \"machine_NN\", \"labour_NN\", \"power_NN\"],\n",
    "           \n",
    "           \"non_work_related\": [\"anger_NN\",\"apple_NN\",\"art_NN\", \"democracy_NN\",\n",
    "                  \"happiness_NN\", \"man_NN\",\n",
    "                  \"nation_NN\", 'woman_NN']\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Optimal time range for a Language Model\n",
    "\n",
    "The following two cells reproduce Figure 1. It finds the optimal date range for the different language models (measured by the F1-score of the positive class) givenn some headwords and using the _sense centroid_ method. Quotations are filtered in moving 100-year time windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_daterange(results_path, lemmas, timestart, year_window, metric):\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    clf_dict = defaultdict(list)\n",
    "    results = {}\n",
    "    csv_files = results_path.glob(\"**/*.csv\")\n",
    "    for csv in csv_files:\n",
    "        current_csv = str(csv).split(\"/\")\n",
    "        current_lemma = current_csv[1]\n",
    "        current_sense = current_csv[3].split(\"~\")[0]\n",
    "        lemma_pickle = pd.read_pickle(\"data/lemma_senses_\" + current_lemma + \".pickle\")\n",
    "        sst = lemma_pickle[lemma_pickle[\"id\"] == current_sense].iloc[0][\"daterange.start\"]\n",
    "            \n",
    "        try:\n",
    "            df = pd.read_csv(csv)\n",
    "            \n",
    "            df = df[[\"label\",\"year\",\"quotation_id\",\"bert_centroid_sense_vector_bert_base_-1,-2,-3,-4_mean\",\n",
    "                     \"bert_centroid_sense_vector_bert_1850_-1,-2,-3,-4_mean\",\"bert_centroid_sense_vector_blert_base_-1,-2,-3,-4_mean\"]]\n",
    "            df = df.rename(columns={\"bert_centroid_sense_vector_bert_base_-1,-2,-3,-4_mean\": \"bert_base_sense_centroid\",\n",
    "                                    \"bert_centroid_sense_vector_bert_1850_-1,-2,-3,-4_mean\": \"bert_1850_sense_centroid\",\n",
    "                                    \"bert_centroid_sense_vector_blert_base_-1,-2,-3,-4_mean\": \"bert_1900_sense_centroid\"})\n",
    "            df = df[df[\"year\"].between(timestart, timestart+(year_window*2))]\n",
    "\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        \n",
    "        if current_lemma in lemmas:\n",
    "            for col in df.columns:\n",
    "                clf_dict[col].extend(df[col])\n",
    "\n",
    "    for colname, classifications in clf_dict.items():\n",
    "        if colname not in ('label','year','quotation_id') and colname.startswith(\"bert_\"):\n",
    "            p,r = [round(x,3) for x in precision_recall_fscore_support(clf_dict['label'],classifications,average='binary',pos_label=1)[:2] if x] \n",
    "            f1 = round((2*(p*r))/(p+r),3)\n",
    "            if metric == \"recall\":\n",
    "                results[colname] = round(r,3)\n",
    "            if metric == \"precision\":\n",
    "                results[colname] = round(p,3)\n",
    "            if metric == \"fscore\":\n",
    "                results[colname] = round(f1,3)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain Figure 1 from the paper, `setting` should be `all_lemmas` and `metric` should be `fscore`.\n",
    "\n",
    "You can experiment further with other scenarios by changing the metrics and groupings of lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = \"all_lemmas\" # Options: see dictionary \"dLemmas\" at the beginning of the notebook.\n",
    "metric = \"fscore\" # Options: \"precision\", \"recall\", \"fscore\" (this always corresponds to the positive class)\n",
    "\n",
    "lemmas = dLemmas[setting]\n",
    "\n",
    "path = Path('figures')\n",
    "path.mkdir(exist_ok=True)\n",
    "\n",
    "time_experiment = 2000 # Folder from which we select results\n",
    "test_daterange_start = 1760 # Quotations starting from\n",
    "year_window = 50\n",
    "bert_base = []\n",
    "bert_1850 = []\n",
    "bert_1900 = []\n",
    "time_mean = []\n",
    "for date_start in range(test_daterange_start, time_experiment - year_window,10):\n",
    "    results = find_optimal_daterange(Path('results_' + str(time_experiment)), lemmas, date_start, year_window, metric)\n",
    "    bert_base.append(results[\"bert_base_sense_centroid\"])\n",
    "    bert_1850.append(results[\"bert_1850_sense_centroid\"])\n",
    "    bert_1900.append(results[\"bert_1900_sense_centroid\"])\n",
    "    time_mean.append(date_start + year_window)\n",
    "    \n",
    "plt.plot(time_mean, bert_base)\n",
    "plt.plot(time_mean, bert_1850)\n",
    "plt.plot(time_mean, bert_1900)\n",
    "\n",
    "plt.legend([\"BERTbase\", \"BERT1850\", \"BERT1900\"], loc='lower right')\n",
    "\n",
    "plt.savefig(path / (\"plot_LM_\" + setting + \"_class1\" + metric + \"_50.png\"), dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Optimal time range for a Language Model: weighted sense centroid\n",
    "\n",
    "This does the same as the preceding two cells, but for the time-sensitive approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_daterange_ts(results_path, lemmas, timestart, year_window, metric, weighting):\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    clf_dict = defaultdict(list)\n",
    "    results = {}\n",
    "    csv_files = results_path.glob(\"**/*.csv\")\n",
    "    for csv in csv_files:\n",
    "        current_csv = str(csv).split(\"/\")\n",
    "        current_lemma = current_csv[1]\n",
    "        current_sense = current_csv[3].split(\"~\")[0]\n",
    "        lemma_pickle = pd.read_pickle(\"data/lemma_senses_\" + current_lemma + \".pickle\")\n",
    "        sst = lemma_pickle[lemma_pickle[\"id\"] == current_sense].iloc[0][\"daterange.start\"]\n",
    "            \n",
    "        try:\n",
    "            df = pd.read_csv(csv)\n",
    "            \n",
    "            df = df[[\"label\",\"year\",\"quotation_id\",\"bert_ts_\" + weighting + \"_centroid_sense_vector_blert_base_-1,-2,-3,-4_mean\",\n",
    "                     \"bert_ts_\" + weighting + \"_centroid_sense_vector_bert_base_-1,-2,-3,-4_mean\",\"bert_ts_\" + weighting + \"_centroid_sense_vector_bert_1850_-1,-2,-3,-4_mean\"]]\n",
    "            df = df.rename(columns={\"bert_ts_\" + weighting + \"_centroid_sense_vector_bert_base_-1,-2,-3,-4_mean\": \"bert_base_sense_centroid_\" + weighting,\n",
    "                                    \"bert_ts_\" + weighting + \"_centroid_sense_vector_bert_1850_-1,-2,-3,-4_mean\": \"bert_1850_sense_centroid_\" + weighting,\n",
    "                                    \"bert_ts_\" + weighting + \"_centroid_sense_vector_blert_base_-1,-2,-3,-4_mean\": \"bert_1900_sense_centroid_\" + weighting})\n",
    "            df = df[df[\"year\"].between(timestart, timestart+(year_window*2))]\n",
    "\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        \n",
    "        if current_lemma in lemmas:\n",
    "            for col in df.columns:\n",
    "                clf_dict[col].extend(df[col])\n",
    "\n",
    "    for colname, classifications in clf_dict.items():\n",
    "        if colname not in ('label','year','quotation_id') and colname.startswith(\"bert_\"):\n",
    "            p,r = [round(x,3) for x in precision_recall_fscore_support(clf_dict['label'],classifications,average='binary',pos_label=1)[:2] if x] \n",
    "            f1 = round((2*(p*r))/(p+r),3)\n",
    "            if metric == \"recall\":\n",
    "                results[colname] = round(r,3)\n",
    "            if metric == \"precision\":\n",
    "                results[colname] = round(p,3)\n",
    "            if metric == \"fscore\":\n",
    "                results[colname] = round(f1,3)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = \"abstract\"\n",
    "weighting = \"weighted\"\n",
    "metric = \"fscore\"\n",
    "\n",
    "lemmas = dLemmas[setting]\n",
    "\n",
    "time_experiment = 1920 # Folder from which we select results: \"1850\" or \"1920\"\n",
    "test_daterange_start = 1760 # Quotations starting from\n",
    "year_window = 50\n",
    "bert_base = []\n",
    "bert_1850 = []\n",
    "bert_1900 = []\n",
    "time_mean = []\n",
    "for date_start in range(test_daterange_start, time_experiment - year_window,10):\n",
    "    results = find_optimal_daterange_ts(Path('results_ts_' + str(time_experiment)), lemmas, date_start, year_window, metric, weighting)\n",
    "    bert_base.append(results[\"bert_base_sense_centroid_\" + weighting])\n",
    "    bert_1850.append(results[\"bert_1850_sense_centroid_\" + weighting])\n",
    "    bert_1900.append(results[\"bert_1900_sense_centroid_\" + weighting])\n",
    "    time_mean.append(date_start + year_window)\n",
    "    \n",
    "plt.plot(time_mean, bert_base)\n",
    "plt.plot(time_mean, bert_1850)\n",
    "plt.plot(time_mean, bert_1900)\n",
    "\n",
    "plt.legend([\"BERTbase\", \"BERT1850\", \"BERT1900\"], loc='lower right')\n",
    "\n",
    "plt.savefig(\"figures/plot_LM_\" + setting + \"_class1\" + metric + \"_50_ts_\" + weighting + \".png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect differences between time-sensitive and standard\n",
    "\n",
    "What do we gain by using the weighted method? And what do we lose with it? Compare quotations correclty labeled by the time-sensitive method and incorrectly labeled by the time-insensitive method, or viceversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_sensitive_exploration(results_path, lemmas, observed_LM, ts_method, improvement):\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    clf_dict = defaultdict(list)\n",
    "    results = {}\n",
    "    csv_files = results_path.glob(\"**/*.csv\")\n",
    "    years = []\n",
    "    years_total = []\n",
    "    for csv in csv_files:\n",
    "        current_csv = str(csv).split(\"/\")\n",
    "        current_lemma = current_csv[1]\n",
    "        current_sense = current_csv[3].split(\"~\")[0]\n",
    "        lemma_pickle = pd.read_pickle(\"data/lemma_senses_\" + current_lemma + \".pickle\")\n",
    "        \n",
    "        if current_lemma in lemmas:\n",
    "            try:\n",
    "                df = pd.read_csv(csv)\n",
    "\n",
    "                df = df[[\"label\",\"year\",\"quotation_id\",\"bert_centroid_sense_vector_\" + observed_LM + \"_-1,-2,-3,-4_mean\",\n",
    "                         \"bert_ts_\" + ts_method + \"_centroid_sense_vector_\" + observed_LM + \"_-1,-2,-3,-4_mean\"]]\n",
    "\n",
    "                df = df.rename(columns={\"bert_centroid_sense_vector_\" + observed_LM + \"_-1,-2,-3,-4_mean\": observed_LM + \"_standard\",\n",
    "                                        \"bert_ts_\" + ts_method + \"_centroid_sense_vector_\" + observed_LM + \"_-1,-2,-3,-4_mean\": observed_LM + \"_\" + ts_method})\n",
    "\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "            filtered_df = pd.DataFrame()\n",
    "            if improvement == \"weighted\":\n",
    "                filtered_df = df[(df[\"label\"] == 1) & (df[observed_LM + \"_standard\"] == 0) & (df[observed_LM + \"_\" + ts_method] == 1)]\n",
    "            elif improvement == \"standard\":\n",
    "                filtered_df = df[(df[\"label\"] == 1) & (df[observed_LM + \"_standard\"] == 1) & (df[observed_LM + \"_\" + ts_method] == 0)]\n",
    "            \n",
    "            years += list(filtered_df[\"year\"].values)\n",
    "            years_total += list(df[df[\"label\"] == 1][\"year\"].values)\n",
    "    \n",
    "    batch1 = [y for y in years if y >= 1770 and y <= 1800]\n",
    "    batch2 = [y for y in years if y > 1800 and y <= 1830]\n",
    "    batch3 = [y for y in years if y > 1830 and y <= 1860]\n",
    "    batch4 = [y for y in years if y > 1860 and y <= 1890]\n",
    "    batch5 = [y for y in years if y > 1890 and y <= 1920]\n",
    "    \n",
    "    batch1_total = [y for y in years_total if y >= 1770 and y <= 1800]\n",
    "    batch2_total = [y for y in years_total if y > 1800 and y <= 1830]\n",
    "    batch3_total = [y for y in years_total if y > 1830 and y <= 1860]\n",
    "    batch4_total = [y for y in years_total if y > 1860 and y <= 1890]\n",
    "    batch5_total = [y for y in years_total if y > 1890 and y <= 1920]\n",
    "    \n",
    "    return len(batch1)/len(batch1_total), len(batch2)/len(batch2_total), len(batch3)/len(batch3_total), len(batch4)/len(batch4_total), len(batch5)/len(batch5_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = \"abstract\"\n",
    "metric = \"fscore\"\n",
    "improvement = \"weighted\" # \"weighted\" to see improvement of weighted over standard\n",
    "                         # \"standard\" to see improvement of standard over weighted\n",
    "\n",
    "lemmas = dLemmas[setting]\n",
    "\n",
    "lms = [\"bert_1850\", \"blert_base\", \"bert_base\"]\n",
    "\n",
    "for improvement in [\"weighted\", \"standard\"]:\n",
    "    print(\"Explore improvements by using:\", improvement)\n",
    "    print(\"==========================================\")\n",
    "    time_experiment = 1920 # Folder from which we select results\n",
    "    # observed_LM = \"bert_1850\"\n",
    "    for observed_LM in lms:\n",
    "        print(\"Setting:\", setting)\n",
    "        print(\"EXP: 1760-\" + str(time_experiment))\n",
    "        print(\"LM:\", observed_LM)\n",
    "        ts_method = \"weighted\"\n",
    "        batches = time_sensitive_exploration(Path('results_ts_' + str(time_experiment)), lemmas, observed_LM, ts_method, improvement)\n",
    "        print(\"\\tBatch 1750-1800:\", batches[0])\n",
    "        print(\"\\tBatch 1800-1850:\", batches[1])\n",
    "        print(\"\\tBatch 1850-1900:\", batches[2])\n",
    "        print(\"\\tBatch 1900-1950:\", batches[3])\n",
    "        print(\"\\tBatch 1950-2000:\", batches[4])\n",
    "        print(\"\\tAll batches:\", statistics.mean(batches))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration\n",
    "\n",
    "Code to generate table 1 describing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.classificaton_utils import binarize\n",
    "\n",
    "words = [['anger',\"NN\"],[\"apple\",\"NN\"],[\"art\",\"NN\"],[\"democracy\",\"NN\"],\n",
    "         [\"happiness\",\"NN\"],[\"labour\",\"NN\"],[\"machine\",\"NN\"],[\"man\",\"NN\"],\n",
    "         [\"nation\",\"NN\"],[\"power\",\"NN\"],[\"slave\",\"NN\"],['woman','NN']]\n",
    "\n",
    "experiment = {\"start\": 1760, \"end\": 1850, \"filter_val\": True, \"filter_test\": True}\n",
    "\n",
    "dr = dict()\n",
    "for lemma, pos in words:\n",
    "    print(\"### lemma: {} ###\".format(lemma))\n",
    "    quotations_path = f\"./data/sfrel_quotations_{lemma}_{pos}.pickle\"\n",
    "    lemma_senses = pd.read_pickle(f'./data/lemma_senses_{lemma}_{pos}.pickle')\n",
    "\n",
    "    senses = set(lemma_senses[lemma_senses.word_id.str.startswith(f'{lemma}_{pos.lower()}')][\"id\"])\n",
    "    relations = ['seed','synonym']\n",
    "    eval_mode = \"lemma_etal\"\n",
    "\n",
    "    df_train, df_val, df_test = binarize(lemma=lemma,\n",
    "                                    pos=pos,\n",
    "                                    senses=senses, \n",
    "                                    start=experiment[\"start\"],\n",
    "                                    end=experiment[\"end\"],\n",
    "                                    relations=relations,\n",
    "                                    eval_mode=eval_mode,\n",
    "                                    filter_val_by_year=experiment[\"filter_val\"],\n",
    "                                    filter_test_by_year=experiment[\"filter_test\"],\n",
    "                                    strict_filter=True)\n",
    "\n",
    "    df_all = pd.concat([df_train, df_val, df_test])\n",
    "    \n",
    "    unique_seed_senses = len(df_all[df_all[\"provenance_type\"] == \"seed\"][\"sense_id\"].unique())\n",
    "    unique_syn_senses = round(len(df_all[df_all[\"provenance_type\"] == \"synonym\"][\"sense_id\"].unique())/unique_seed_senses)\n",
    "    unique_other_senses = round(len(df_all[~df_all[\"provenance_type\"].isin([\"synonym\", \"seed\"])][\"sense_id\"].unique())/unique_seed_senses)\n",
    "    df_all_posq = df_all[df_all[\"label\"] == \"1\"]\n",
    "    df_all_negq = df_all[df_all[\"label\"] == \"0\"]\n",
    "    quotations_p = round(len(df_all_posq.quotation_id.unique())/unique_seed_senses)\n",
    "    quotations_n = round(len(df_all_negq.quotation_id.unique())/unique_seed_senses)\n",
    "    quotations = str(quotations_p) + \"/\" + str(quotations_n)\n",
    "    derived_senses = str(unique_syn_senses) + \"/\" + str(unique_other_senses)\n",
    "     \n",
    "    dr[lemma] = [unique_seed_senses, derived_senses, quotations]\n",
    "    \n",
    "data_description = pd.DataFrame.from_dict(dr, orient='index', columns=['Seeds','ExpSenses', 'Quotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_description.to_latex())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py37histdict)",
   "language": "python",
   "name": "py37histdict"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
