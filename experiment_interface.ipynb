{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset_download import *\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path, PosixPath\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import API credentials\n",
    "with open('../oed_experiments/oed_credentials.json') as f:\n",
    "    credentials = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lemma\n",
    "lemma_id = \"machine_nn01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path(dp)\n",
    "save_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query the API and get the json response\n",
    "sense_json = query_oed(credentials,'word',lemma_id,flags='include_senses=true&include_quotations=true')\n",
    "\n",
    "# convert the json in a dataframe\n",
    "senses_df = convert_json_to_dataframe(sense_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe\n",
    "# as pickle\n",
    "senses_df.to_pickle(save_path / f\"senses_{lemma_id}.pickle\")\n",
    "# as csv\n",
    "senses_df.to_csv(save_path / f\"senses_{lemma_id}.tsv\",sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open pickle file to avoid calling the API again\n",
    "with open(save_path / f\"senses_{lemma_id}.pickle\",'rb') as in_pickle:\n",
    "    machine_senses_df = pickle.load(in_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all senses that are siblings and descendants\n",
    "# of the semantic class of senses listed in previously obtained query \n",
    "responses = traverse_thesaurus(credentials,machine_senses_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traverse tree or load responses \n",
    "# responses = traverse_thesaurus(credentials,machine_senses_df)\n",
    "with open(f'{dp}/tree_traversal.pickle','rb') as in_pickle:\n",
    "    responses = pickle.load(in_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all quoations for the senses in the responses variable\n",
    "quotations = get_quotations_from_thesaurus(credentials,responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge and save all information stored in the seperate pickle files\n",
    "df = merge_pickled(Path(\"./data/senses_machine_nn01.pickle\"),\n",
    "                   Path(\"./data/tree_traversal.pickle\"),\n",
    "                   Path(\"./data/tree_traversal_quotations.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(f\"{dp}/{lemma_id}_all.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* \u001b[32m19-machine-tagger\u001b[m\r\n",
      "  dev\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!git branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start,end = 1750,1950\n",
    "lemma_id = 'machine_nn01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_last_id = lambda nested_list :[l[-1] for l in nested_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_from_lemma_query(auth,lemma_id,start=1750,end=1950):\n",
    "    \"\"\"Extends senses from a dataframe generate from accessing\n",
    "    the API via the word endpoint. The script first retrieves all\n",
    "    senses, then synonyms for these senses, then other senses that \n",
    "    match the semantic classes of the retrieved senses.\n",
    "    \n",
    "    This script also aims to record the \"provenance\" of words, \n",
    "    their relation to the initial query, which can help to \n",
    "    select of filter words later on.\n",
    "    \n",
    "    Arguments:\n",
    "        lemma_id (str)\n",
    "        start (int)\n",
    "        end (int)\n",
    "    Returns\n",
    "        a pandas.DataFrame\n",
    "    \"\"\"\n",
    "    # load seed query dataframe\n",
    "    query_df = pd.read_pickle(f\"./data/senses_{lemma_id}.pickle\")\n",
    "    \n",
    "    # use the sense endpoint to ensure all information \n",
    "    # can be properly concatenated in one dataframe\n",
    "    \n",
    "    # retrieve all sense ids\n",
    "    query_sense_ids = query_df.id.unique()\n",
    "    \n",
    "    # get all senses by sense id\n",
    "    print(f\"Get all sense for the lemma {lemma_id}\")\n",
    "    seeds = [(s,query_oed(auth,'sense',s,\n",
    "                    flags=f\"current_in='{start}-{end}'&limit=1000\"))\n",
    "                        for s in tqdm(query_sense_ids)]\n",
    "    \n",
    "    # convert to dataframe\n",
    "    seeds_df = pd.DataFrame([seed['data'] for s_id,seed in seeds])\n",
    "    \n",
    "    # define provenance, these words are \"seed\"\n",
    "    seeds_df['provenance'] = seeds_df.id\n",
    "    seeds_df['provenance_type'] = 'seed'\n",
    "    \n",
    "    # get all synonyms for the seed senses\n",
    "    print(f\"Get all synonyms of the senses listed in {lemma_id}\")\n",
    "    synonyms = [(s,query_oed(auth,'sense',s,\n",
    "                    level='synonyms',\n",
    "                    flags=f\"current_in='{start}-{end}'&limit=1000\"))\n",
    "                            for s in tqdm(query_sense_ids)]\n",
    "\n",
    "    # transform list of synonyms to a dataframe\n",
    "    synonyms_df = pd.DataFrame([s for s_id,syn in synonyms for s in syn['data']])\n",
    "    \n",
    "    # these items have provenancy type \"synonym\"\n",
    "    synonyms_df['provenance'] = [s_id for s_id,syn in synonyms for s in syn['data']]\n",
    "    synonyms_df['provenance_type'] = 'synonym'\n",
    "    \n",
    "    # seed + synonyms constitute the nucleas of our query\n",
    "    # branch from there\n",
    "    core_df = pd.concat([seeds_df,synonyms_df])\n",
    "    core_df['semantic_class_last_id'] = core_df['semantic_class_ids'].apply(get_last_id)\n",
    "    \n",
    "    # retrieve all semantic class ids for the senses so far\n",
    "    semantic_class_ids = set([s for l in core_df.semantic_class_last_id.to_list() for s in l])\n",
    "\n",
    "    # get all the branches for the retrieve semantic class ids\n",
    "    print(\"Get all branches for seed senses and synonyms\")\n",
    "    branches = [(idx,query_oed(auth,'semanticclass', idx, \n",
    "                        level='branchsenses',\n",
    "                        flags=f\"current_in='{start}-{end}'&limit=1000\"))\n",
    "                            for idx in tqdm(semantic_class_ids)]\n",
    "    \n",
    "    # convert API response to dataframe\n",
    "    branches_df = pd.DataFrame([s for idx,branch in branches for s in branch['data']])\n",
    "    \n",
    "    # provenance_type is branch with semantic class id \n",
    "    # that was use for retrieving the sense is the provenance\n",
    "    branches_df['provenance'] = [idx for idx,branch in branches for s in branch['data']]\n",
    "    branches_df['provenance_type'] = 'branch'\n",
    "    \n",
    "    branches_df['semantic_class_last_id'] = branches_df.semantic_class_ids.apply(get_last_id)\n",
    "    \n",
    "    # remove senses that already appear in the core_df\n",
    "    branches_df_red = branches_df.loc[~branches_df.id.isin(core_df.id)]\n",
    "    \n",
    "    # concatenate core and branch senses\n",
    "    extended_df = pd.concat([core_df,branches_df_red])\n",
    "    \n",
    "    # refine the provenance type\n",
    "    # if the last semantic class id is not equal to provenance\n",
    "    # this row is a child or descendant\n",
    "    check_membership = lambda row : row.provenance in row.semantic_class_last_id\n",
    "    extended_df.loc[(~extended_df.apply(check_membership,axis=1)) & (extended_df.provenance_type=='branch'),\n",
    "                [\"provenance_type\"]]  = \"branch_descendant\"\n",
    "    \n",
    "    # save information\n",
    "    extended_df.to_pickle(f'{dp}/senses_{lemma_id}_extended.pickle')\n",
    "    return extended_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_df = extend_from_lemma_query(credentials,lemma_id,start,end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"Aaron's rod\",\n",
       " 'Durex',\n",
       " 'French letter',\n",
       " 'Frenchy',\n",
       " \"God's image\",\n",
       " 'Jacuzzi',\n",
       " 'John',\n",
       " 'John Henry',\n",
       " 'John Thomas',\n",
       " 'Johnny',\n",
       " 'Johnson',\n",
       " 'Lizzie',\n",
       " 'M.F.V.',\n",
       " 'Mudian',\n",
       " 'Percy',\n",
       " 'Peter',\n",
       " 'Roger',\n",
       " 'Trojan',\n",
       " 'Turing machine',\n",
       " 'address',\n",
       " 'aerobat',\n",
       " 'aerocar',\n",
       " 'aerostat',\n",
       " 'affair',\n",
       " 'air machine',\n",
       " 'air vessel',\n",
       " 'aircraft',\n",
       " 'anatomy',\n",
       " 'appliance',\n",
       " 'ark',\n",
       " 'armour',\n",
       " 'arrangement',\n",
       " 'arrow',\n",
       " 'art',\n",
       " 'artifice',\n",
       " 'automaton',\n",
       " 'avion',\n",
       " 'baby-maker',\n",
       " 'barge',\n",
       " 'bark',\n",
       " 'bastiment',\n",
       " 'bathing-machine',\n",
       " 'beam',\n",
       " 'being',\n",
       " 'belly',\n",
       " 'bike',\n",
       " 'biocomputer',\n",
       " 'blood bulk',\n",
       " 'board',\n",
       " 'bodiȝlich',\n",
       " 'body',\n",
       " 'bone house',\n",
       " 'bones',\n",
       " 'bottom',\n",
       " 'bouk',\n",
       " 'bubble',\n",
       " 'buggy',\n",
       " 'bulk',\n",
       " 'bus',\n",
       " 'buzz-box',\n",
       " 'buzz-wagon',\n",
       " 'cabinet',\n",
       " 'cade',\n",
       " 'cadre',\n",
       " 'car',\n",
       " 'carcass',\n",
       " 'carriage',\n",
       " 'carrion',\n",
       " 'casa',\n",
       " 'case',\n",
       " 'cast',\n",
       " 'cautel',\n",
       " 'chariot',\n",
       " 'chassis',\n",
       " 'chevisance',\n",
       " 'chode',\n",
       " 'class',\n",
       " 'clay',\n",
       " 'clipper',\n",
       " 'clod',\n",
       " 'cock',\n",
       " 'cod',\n",
       " 'compass',\n",
       " 'compassing',\n",
       " 'compilement',\n",
       " 'composition',\n",
       " 'compound',\n",
       " 'condom',\n",
       " 'conject',\n",
       " 'conjecture',\n",
       " 'contignation',\n",
       " 'contraption',\n",
       " 'contrevure',\n",
       " 'contrival',\n",
       " 'contrivance',\n",
       " 'contrivement',\n",
       " 'convenience',\n",
       " 'conveniency',\n",
       " 'conveyance',\n",
       " 'corporation',\n",
       " 'corporeity',\n",
       " 'corpse',\n",
       " 'corpus',\n",
       " 'corse',\n",
       " 'cost',\n",
       " 'craft',\n",
       " 'cran',\n",
       " 'cunning',\n",
       " 'cust',\n",
       " 'cycle',\n",
       " 'device',\n",
       " 'dial',\n",
       " 'dick',\n",
       " 'dicky',\n",
       " 'dildo',\n",
       " 'diligence',\n",
       " 'dilly',\n",
       " 'ding-a-ling',\n",
       " 'dodge',\n",
       " 'dog',\n",
       " 'dong',\n",
       " 'dork',\n",
       " 'drag',\n",
       " 'drum',\n",
       " 'dust',\n",
       " 'dwelling-house',\n",
       " 'earth',\n",
       " 'easel-picture',\n",
       " 'easel-piece',\n",
       " 'embarkation',\n",
       " 'embodiment',\n",
       " 'energizer',\n",
       " 'engine',\n",
       " 'excogitation',\n",
       " 'expediency',\n",
       " 'expedient',\n",
       " 'expediment',\n",
       " 'fabric',\n",
       " 'fabrication',\n",
       " 'fancy',\n",
       " 'fanglement',\n",
       " 'fashion',\n",
       " 'ficelle',\n",
       " 'fidcock',\n",
       " 'finding',\n",
       " 'fire engine',\n",
       " 'firehouse',\n",
       " 'flesh',\n",
       " 'flesh-rind',\n",
       " 'flesh-stuff',\n",
       " 'flood-bickerer',\n",
       " 'flute',\n",
       " 'fly',\n",
       " 'flyer',\n",
       " 'flying post',\n",
       " 'forcing-engine',\n",
       " 'frame',\n",
       " 'framework',\n",
       " 'gadget',\n",
       " 'generator',\n",
       " 'gimcrack',\n",
       " 'gin',\n",
       " 'golem',\n",
       " 'graith',\n",
       " 'gun',\n",
       " 'handle',\n",
       " 'high-flyer',\n",
       " 'home',\n",
       " 'hooker',\n",
       " 'horse',\n",
       " 'house',\n",
       " 'housing',\n",
       " 'implements',\n",
       " 'incarnation',\n",
       " 'industry',\n",
       " 'ingenuity',\n",
       " 'ingeny',\n",
       " 'instrument',\n",
       " 'intromittent',\n",
       " 'invent',\n",
       " 'invention',\n",
       " 'jam-jar',\n",
       " 'jet',\n",
       " 'jigamaree',\n",
       " 'jock',\n",
       " 'kakemono',\n",
       " 'keel',\n",
       " 'khazi',\n",
       " 'kink',\n",
       " 'kipsie',\n",
       " 'knack',\n",
       " 'knob',\n",
       " 'langer',\n",
       " 'lay',\n",
       " 'length',\n",
       " 'letter',\n",
       " 'lich',\n",
       " 'licham',\n",
       " 'lock',\n",
       " 'longhouse',\n",
       " 'loom',\n",
       " 'low rider',\n",
       " 'lump of clay',\n",
       " 'lurk',\n",
       " 'machina',\n",
       " 'machinament',\n",
       " 'machination',\n",
       " 'machine for living',\n",
       " 'machine power',\n",
       " 'machinery',\n",
       " 'mail coach',\n",
       " 'mail packet',\n",
       " 'mail stage',\n",
       " 'mail-hack',\n",
       " 'maison',\n",
       " 'man',\n",
       " 'man-machine',\n",
       " 'management',\n",
       " 'manhood',\n",
       " 'manoeuvre',\n",
       " 'mansion house',\n",
       " 'meat tool',\n",
       " 'mechanical advantage',\n",
       " 'mechanism',\n",
       " 'membrum virile',\n",
       " 'mentula',\n",
       " 'mickey',\n",
       " 'micky',\n",
       " 'microcosm',\n",
       " 'micrograph',\n",
       " 'middle leg',\n",
       " 'mister',\n",
       " 'mole',\n",
       " 'molition',\n",
       " 'monkey',\n",
       " 'motor',\n",
       " 'motor car',\n",
       " 'motor carriage',\n",
       " 'move',\n",
       " 'mover',\n",
       " 'mud wall',\n",
       " 'nag',\n",
       " 'nanny',\n",
       " 'nanny-goat',\n",
       " 'needle',\n",
       " 'nerve',\n",
       " 'noonies',\n",
       " 'nymph',\n",
       " 'old fellow',\n",
       " 'old man',\n",
       " 'organ',\n",
       " 'other thing',\n",
       " 'oustil',\n",
       " 'outwall',\n",
       " 'outward man',\n",
       " 'pageant',\n",
       " 'passage',\n",
       " 'pata',\n",
       " 'pecker',\n",
       " 'pee-pee',\n",
       " 'pego',\n",
       " 'pencil',\n",
       " 'penis',\n",
       " 'person',\n",
       " 'personage',\n",
       " 'personality',\n",
       " 'pillicock',\n",
       " 'pillock',\n",
       " 'pine',\n",
       " 'pintle',\n",
       " 'pisser',\n",
       " 'pitchboard',\n",
       " 'pizzle',\n",
       " 'plant',\n",
       " 'ploy',\n",
       " 'poker',\n",
       " 'police',\n",
       " 'policy',\n",
       " 'pork',\n",
       " 'port',\n",
       " 'post calash',\n",
       " 'post vehicle',\n",
       " 'post-car',\n",
       " 'post-carriage',\n",
       " 'post-coach',\n",
       " 'post-equipage',\n",
       " 'post-stage',\n",
       " 'post-wagon',\n",
       " 'power',\n",
       " 'prependent',\n",
       " 'prick',\n",
       " 'prime mover',\n",
       " 'primum mobile',\n",
       " 'privy',\n",
       " 'prong',\n",
       " 'propagator',\n",
       " 'prophylactic',\n",
       " 'prore',\n",
       " 'protective',\n",
       " 'prow',\n",
       " 'pud',\n",
       " 'pudding',\n",
       " 'putz',\n",
       " 'quadrant',\n",
       " 'quaint',\n",
       " 'quaintise',\n",
       " 'quarrons',\n",
       " 'quiff',\n",
       " 'quillity',\n",
       " 'rammer',\n",
       " 'recipe',\n",
       " 'ride',\n",
       " 'rig',\n",
       " 'robot',\n",
       " 'roof',\n",
       " 'root',\n",
       " 'rubber',\n",
       " 'rubigo',\n",
       " 'ruler',\n",
       " 'runnion',\n",
       " 'safe',\n",
       " 'safety',\n",
       " 'schlong',\n",
       " 'scoot',\n",
       " 'scooter',\n",
       " 'scroll painting',\n",
       " 'scroll picture',\n",
       " 'scumbag',\n",
       " 'sensuality',\n",
       " 'sex',\n",
       " 'shack',\n",
       " 'shaft',\n",
       " 'sheath',\n",
       " 'shebang',\n",
       " 'shift',\n",
       " 'ship',\n",
       " 'short',\n",
       " 'skift',\n",
       " 'skinful',\n",
       " 'slime',\n",
       " 'smoke-house',\n",
       " 'soma',\n",
       " 'soul case',\n",
       " 'spell',\n",
       " 'stage',\n",
       " 'stage-wagon',\n",
       " 'stagecoach',\n",
       " 'stager',\n",
       " 'standing house',\n",
       " 'steer',\n",
       " 'stern-bearer',\n",
       " 'stick',\n",
       " 'stroke',\n",
       " 'structure',\n",
       " 'subtility',\n",
       " 'sulky',\n",
       " 'swill',\n",
       " 'swipe',\n",
       " 'tabernacle',\n",
       " 'tadger',\n",
       " 'tally-ho',\n",
       " 'tarse',\n",
       " 'tondo',\n",
       " 'tonk',\n",
       " 'tool',\n",
       " 'totalizator',\n",
       " 'totalizer',\n",
       " 'totalizing',\n",
       " 'tote',\n",
       " 'tour',\n",
       " 'traction',\n",
       " 'trade',\n",
       " 'tram',\n",
       " 'transportation',\n",
       " 'tree',\n",
       " 'trick',\n",
       " 'triumphal',\n",
       " 'trunk',\n",
       " 'tub',\n",
       " 'tube',\n",
       " 'utensil',\n",
       " 'utter man',\n",
       " 'vehicle',\n",
       " 'vehiculum',\n",
       " 'verge',\n",
       " 'vessel',\n",
       " 'vice',\n",
       " 'virge',\n",
       " 'virile member',\n",
       " 'virilia',\n",
       " 'voiture',\n",
       " 'wagon',\n",
       " 'wagon-coach',\n",
       " 'wanker',\n",
       " 'water engine',\n",
       " 'water house',\n",
       " 'water treader',\n",
       " 'watercraft',\n",
       " 'weapon',\n",
       " 'wee-wee',\n",
       " 'whang',\n",
       " 'whanger',\n",
       " 'wheel',\n",
       " 'wheels',\n",
       " 'whore-pipe',\n",
       " 'wile',\n",
       " 'willy',\n",
       " 'winkle',\n",
       " 'winky',\n",
       " 'wooden horse',\n",
       " 'wooden isle',\n",
       " 'work-loom',\n",
       " 'wrinkle',\n",
       " 'yard'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(merged[(merged.provenance_type=='synonym') & (merged.lemma!='machine')].lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quotations(lemma_id,sense_df=''):\n",
    "    \"\"\"Obtain and store all quotations from a dataframe constructed\n",
    "    from information retrieved via de sense endpoint.\n",
    "    \n",
    "    Arguments:\n",
    "        sense_df (pandas.DataFrame,str)\n",
    "        lemma_id (str)\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame\n",
    "    \"\"\"\n",
    "    if isinstance(sense_df,str):\n",
    "        quotations_df = pd.read_pickle(f'./data/quotations_{lemma_id}.pickle')\n",
    "    else:\n",
    "        quotations = [query_oed(credentials,'sense',sense_idx,level='quotations')\n",
    "                        for sense_idx in tqdm(set(sense_df.id))]\n",
    "            \n",
    "        quotations_df = pd.concat([pd.DataFrame(q['data']) for q in quotations])\n",
    "        quotations_df.to_pickle(f'{dp}/quotations_{lemma_id}.pickle')\n",
    "    return quotations_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotations_df = get_quotations(lemma_id)\n",
    "quotations_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = extended_df.merge(quotations_df[['id','sense_id',\"text\",\"year\",\"source\"]],left_on='id',right_on='sense_id',suffixes=['',\"_quotation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with seed and synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_forms = set(extended_df[extended_df.provenance_type.isin(['seed','synonym'])].lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma_content_from_surface_form(surface_forms):\n",
    "    \"\"\"Get all quotations and senses from a list of surface forms\n",
    "    \"\"\"\n",
    "    lemmatized = [query_oed(credentials,'lemmatize','',flags=f\"form={sf}\")\n",
    "                             for sf in tqdm(surface_forms)]\n",
    "    \n",
    "    lemmas_content = [query_oed(credentials, 'word',\n",
    "                       l['word']['id'], \n",
    "                       flags=\"include_senses=true&include_quotations=true\") \n",
    "                             for lemma in tqdm(lemmatized)\n",
    "                                 for l in lemma['data']]\n",
    "    \n",
    "    lemmas_content_df = pd.DataFrame(sense for lemma in lemmas_content for sense in lemma['data']['senses'])\n",
    "    \n",
    "    quotations = [quotation\n",
    "                        for i,row in lemmas_content_df.iterrows()\n",
    "                             for quotation in row.quotations]\n",
    "    \n",
    "    related_quotations_df = pd.DataFrame(quotations)\n",
    "    \n",
    "    related_merged_df = lemmas_content_df.merge(related_quotations_df[['id','sense_id',\"text\",\"year\",\"source\"]],left_on='id',right_on='sense_id',suffixes=['',\"_quotation\"])\n",
    "    \n",
    "    related_merged_df['provenance'] = None\n",
    "    related_merged_df['provenance_type'] = 'related'\n",
    "    #related_merged_df.loc[related_merged_df.id.isin(merged_df.id),['provenance_type']] = 'in_core'\n",
    "\n",
    "    return related_merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start here if data is downloaded and put into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_merged_df = pd.read_pickle(f'{dp}/related_merged_{lemma_id}.pickle')\n",
    "merged = pd.read_pickle(f'{dp}/merged_{lemma_id}.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#related_merged_df.to_pickle(f'./data/related_merged_{lemma_id}.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2975, 24) (86977, 23)\n"
     ]
    }
   ],
   "source": [
    "core_df = merged[merged.provenance_type.isin(['seed','synonym'])]\n",
    "related_df = related_merged_df.loc[~related_merged_df.id.isin(core_df.id)]\n",
    "\n",
    "print(core_df.shape,related_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_senses = {'machine_nn01-38474233','machine_nn01-38474548','machine_nn01-38475164','machine_nn01-38475286',\n",
    "          \"machine_nn01-38474607\",\"machine_nn01-38475923\",\"machine_nn01-38474877\",\"machine_nn01-38475046\",\n",
    "          \"machine_nn01-38475099\"\n",
    "         }\n",
    "\n",
    "def select_senses(df,list_senses):\n",
    "    sc_ids = set([s for l in df.loc[df.id.isin(list_senses)].semantic_class_last_id.to_list() for s in l])\n",
    "    \n",
    "    overlap = lambda x,l: bool(set(x).intersection(l))\n",
    "    \n",
    "    df = df.loc[(df.id.isin(list_senses))  | (df.provenance.isin(list_senses)) | (df.provenance.isin(sc_ids))]\n",
    "    \n",
    "    return df\n",
    "\n",
    "core_selected_df = select_senses(core_df,include_senses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2975, 24) (1113, 24) (1833, 24)\n"
     ]
    }
   ],
   "source": [
    "core_not_selected_df =  core_df.loc[~core_df.id.isin(core_selected_df.id)]\n",
    "print(core_df.shape,core_selected_df.shape,core_not_selected_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8698, 23)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_sample_df = related_df.sample(frac=.1)\n",
    "\n",
    "# subsampling here!! remove later\n",
    "related_sample_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py37torch/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/data/anaconda/envs/py37torch/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "related_sample_df['label'] = 'not_machine'\n",
    "core_not_selected_df['label'] = 'not_machine'\n",
    "core_selected_df['label'] = 'machine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import re\n",
    "    \n",
    "def preprocess_sent(sent: str) -> str: # check if setting sent_id to None effects anything?\n",
    "    \"\"\"preprocessing function for formatting raw text before training word2vec\n",
    "    # Credits: Kasra Hosseini and Kaspar Beelen\n",
    "    Arguments:\n",
    "        sent (string): input sentence\n",
    "        sent_id (string): idx of the inpute sentence\n",
    "        tokenized (boolean): if True then return the string as a list of tokens\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    # --- replace .- and . in the middle of the word\n",
    "    sent = re.sub(r'(?<=\\w)(\\.-)(?=\\w)', '-', sent)\n",
    "    sent = re.sub(r'(?<=\\w)(\\.)(?=\\w)', '', sent)\n",
    "    # --- remove accent\n",
    "    sent = unidecode.unidecode(sent)\n",
    "    # --- remove 2 or more .\n",
    "    sent = re.sub(r'[.]{2,}', '.', sent)\n",
    "    # --- add a space before and after a list of punctuations\n",
    "    sent = re.sub(r\"([.,!?:;\\\"\\'])\", r\" \\1 \", sent)\n",
    "    # --- remove everything except:\n",
    "    sent = re.sub(r\"([^a-zA-Z\\-.:;,!?\\d+]+)\", r\" \", sent)\n",
    "    # --- replace numbers with <NUM>\n",
    "    sent = re.sub(r'\\b\\d+\\b', '<NUM>', sent)\n",
    "    sent = re.sub(r'--', '', sent)\n",
    "    # --- normalize white spaces\n",
    "    sent = re.sub(r'\\s+', ' ', sent)\n",
    "    # --- lowercase\n",
    "    sent = sent.lower()\n",
    "\n",
    "    \n",
    "    return sent    \n",
    "\n",
    "def process_for_classification(text_col):\n",
    "    sentence = preprocess_sent(text_col[\"full_text\"])\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "combined_all = pd.concat([related_sample_df[['text','label']],\n",
    "                          core_not_selected_df[['text','label']],\n",
    "                          core_selected_df[['text','label']],\n",
    "                         ])\n",
    "combined_all['processed_text'] = combined_all.text.apply(process_for_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_all.drop(combined_all[combined_all.processed_text==''].index,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_all['partition'] = [np.random.choice(['train','dev','test'], p=[0.6, 0.2, 0.2]) for _ in range(combined_all.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in ['train','dev','test']:\n",
    "    combined_all.loc[combined_all.partition==split][['processed_text','label']].to_csv(f'../data/{split}.csv',sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py37torch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/data/anaconda/envs/py37torch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/data/anaconda/envs/py37torch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/data/anaconda/envs/py37torch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/data/anaconda/envs/py37torch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/data/anaconda/envs/py37torch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-06 15:14:32,672 Reading data from ../data\n",
      "2020-10-06 15:14:32,673 Train: ../data/train.csv\n",
      "2020-10-06 15:14:32,673 Dev: ../data/dev.csv\n",
      "2020-10-06 15:14:32,674 Test: ../data/test.csv\n",
      "2020-10-06 15:14:32,702 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9315/9315 [00:02<00:00, 4176.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-06 15:14:35,373 [b'not_machine', b'machine']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import CSVClassificationCorpus\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "path = Path(dp)\n",
    "\n",
    "# column format indicating which columns hold the text and label(s)\n",
    "columns = {1: 'text', 2: 'label'}\n",
    "\n",
    "# load corpus containing training, test and dev data and if CSV has a header, you can skip it\n",
    "corpus: Corpus = CSVClassificationCorpus(path,\n",
    "                                         columns,\n",
    "                                         skip_header=True,\n",
    "                                         delimiter='\\t',    # tab-separated files\n",
    "                                            ) \n",
    "    \n",
    "\n",
    "# 2. create the label dictionary\n",
    "label_dict = corpus.make_label_dictionary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c51f8d38d66b4bddbaf3dfb68c51cd74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9224614c0f461da3be51a31ba248ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=267967963.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-10-06 15:14:46,840 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:14:46,841 Model: \"TextClassifier(\n",
      "  (document_embeddings): TransformerDocumentEmbeddings(\n",
      "    (model): DistilBertModel(\n",
      "      (embeddings): Embeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (transformer): Transformer(\n",
      "        (layer): ModuleList(\n",
      "          (0): TransformerBlock(\n",
      "            (attention): MultiHeadSelfAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (ffn): FFN(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            )\n",
      "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "          (1): TransformerBlock(\n",
      "            (attention): MultiHeadSelfAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (ffn): FFN(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            )\n",
      "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "          (2): TransformerBlock(\n",
      "            (attention): MultiHeadSelfAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (ffn): FFN(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            )\n",
      "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "          (3): TransformerBlock(\n",
      "            (attention): MultiHeadSelfAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (ffn): FFN(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            )\n",
      "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "          (4): TransformerBlock(\n",
      "            (attention): MultiHeadSelfAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (ffn): FFN(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            )\n",
      "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "          (5): TransformerBlock(\n",
      "            (attention): MultiHeadSelfAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (ffn): FFN(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            )\n",
      "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2020-10-06 15:14:46,842 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:14:46,843 Corpus: \"Corpus: 6992 train + 2281 dev + 2323 test sentences\"\n",
      "2020-10-06 15:14:46,843 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:14:46,844 Parameters:\n",
      "2020-10-06 15:14:46,844  - learning_rate: \"3e-05\"\n",
      "2020-10-06 15:14:46,845  - mini_batch_size: \"16\"\n",
      "2020-10-06 15:14:46,845  - patience: \"3\"\n",
      "2020-10-06 15:14:46,845  - anneal_factor: \"0.5\"\n",
      "2020-10-06 15:14:46,846  - max_epochs: \"5\"\n",
      "2020-10-06 15:14:46,846  - shuffle: \"True\"\n",
      "2020-10-06 15:14:46,847  - train_with_dev: \"False\"\n",
      "2020-10-06 15:14:46,847  - batch_growth_annealing: \"False\"\n",
      "2020-10-06 15:14:46,848 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:14:46,848 Model training base path: \"resources/classifier/machine\"\n",
      "2020-10-06 15:14:46,848 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:14:46,849 Device: cuda:0\n",
      "2020-10-06 15:14:46,849 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:14:46,850 Embeddings storage mode: cpu\n",
      "2020-10-06 15:14:46,852 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:15:08,959 epoch 1 - iter 43/437 - loss 0.38572269 - samples/sec: 31.64 - lr: 0.000030\n",
      "2020-10-06 15:15:30,474 epoch 1 - iter 86/437 - loss 0.38741832 - samples/sec: 32.19 - lr: 0.000030\n",
      "2020-10-06 15:15:52,271 epoch 1 - iter 129/437 - loss 0.35730449 - samples/sec: 31.77 - lr: 0.000030\n",
      "2020-10-06 15:16:13,921 epoch 1 - iter 172/437 - loss 0.33781497 - samples/sec: 32.00 - lr: 0.000030\n",
      "2020-10-06 15:16:35,778 epoch 1 - iter 215/437 - loss 0.31716943 - samples/sec: 31.71 - lr: 0.000030\n",
      "2020-10-06 15:16:57,395 epoch 1 - iter 258/437 - loss 0.29632489 - samples/sec: 32.02 - lr: 0.000030\n",
      "2020-10-06 15:17:18,983 epoch 1 - iter 301/437 - loss 0.27955789 - samples/sec: 32.10 - lr: 0.000030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-06 15:17:41,268 epoch 1 - iter 344/437 - loss 0.27381029 - samples/sec: 32.24 - lr: 0.000030\n",
      "2020-10-06 15:18:02,719 epoch 1 - iter 387/437 - loss 0.26759013 - samples/sec: 32.29 - lr: 0.000030\n",
      "2020-10-06 15:18:24,260 epoch 1 - iter 430/437 - loss 0.26022790 - samples/sec: 32.13 - lr: 0.000030\n",
      "2020-10-06 15:18:27,814 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:18:27,815 EPOCH 1 done: loss 0.2574 - lr 0.0000300\n",
      "2020-10-06 15:18:50,179 DEV : loss 0.19068484008312225 - score 0.9325\n",
      "2020-10-06 15:18:51,315 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-10-06 15:18:51,674 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:19:14,627 epoch 2 - iter 43/437 - loss 0.08855720 - samples/sec: 30.54 - lr: 0.000030\n",
      "2020-10-06 15:19:37,148 epoch 2 - iter 86/437 - loss 0.11388903 - samples/sec: 30.78 - lr: 0.000030\n",
      "2020-10-06 15:19:59,808 epoch 2 - iter 129/437 - loss 0.11864037 - samples/sec: 30.53 - lr: 0.000030\n",
      "2020-10-06 15:20:21,648 epoch 2 - iter 172/437 - loss 0.13959469 - samples/sec: 31.73 - lr: 0.000030\n",
      "2020-10-06 15:20:43,522 epoch 2 - iter 215/437 - loss 0.13491325 - samples/sec: 31.65 - lr: 0.000030\n",
      "2020-10-06 15:21:05,108 epoch 2 - iter 258/437 - loss 0.12913395 - samples/sec: 32.11 - lr: 0.000030\n",
      "2020-10-06 15:21:27,792 epoch 2 - iter 301/437 - loss 0.12468893 - samples/sec: 32.06 - lr: 0.000030\n",
      "2020-10-06 15:21:49,302 epoch 2 - iter 344/437 - loss 0.11890957 - samples/sec: 32.15 - lr: 0.000030\n",
      "2020-10-06 15:22:10,756 epoch 2 - iter 387/437 - loss 0.12512137 - samples/sec: 32.28 - lr: 0.000030\n",
      "2020-10-06 15:22:32,313 epoch 2 - iter 430/437 - loss 0.12205680 - samples/sec: 32.14 - lr: 0.000030\n",
      "2020-10-06 15:22:35,866 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:22:35,867 EPOCH 2 done: loss 0.1202 - lr 0.0000300\n",
      "2020-10-06 15:22:58,767 DEV : loss 0.2303922325372696 - score 0.9351\n",
      "2020-10-06 15:22:59,905 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-10-06 15:23:02,284 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:23:24,177 epoch 3 - iter 43/437 - loss 0.04608177 - samples/sec: 32.02 - lr: 0.000030\n",
      "2020-10-06 15:23:45,691 epoch 3 - iter 86/437 - loss 0.03392194 - samples/sec: 32.20 - lr: 0.000030\n",
      "2020-10-06 15:24:07,268 epoch 3 - iter 129/437 - loss 0.03061266 - samples/sec: 32.07 - lr: 0.000030\n",
      "2020-10-06 15:24:28,958 epoch 3 - iter 172/437 - loss 0.03489332 - samples/sec: 31.91 - lr: 0.000030\n",
      "2020-10-06 15:24:51,618 epoch 3 - iter 215/437 - loss 0.03968383 - samples/sec: 32.11 - lr: 0.000030\n",
      "2020-10-06 15:25:13,276 epoch 3 - iter 258/437 - loss 0.03811732 - samples/sec: 32.00 - lr: 0.000030\n",
      "2020-10-06 15:25:34,771 epoch 3 - iter 301/437 - loss 0.04768440 - samples/sec: 32.22 - lr: 0.000030\n",
      "2020-10-06 15:25:56,187 epoch 3 - iter 344/437 - loss 0.04577238 - samples/sec: 32.32 - lr: 0.000030\n",
      "2020-10-06 15:26:17,641 epoch 3 - iter 387/437 - loss 0.04111361 - samples/sec: 32.31 - lr: 0.000030\n",
      "2020-10-06 15:26:39,082 epoch 3 - iter 430/437 - loss 0.05544702 - samples/sec: 32.30 - lr: 0.000030\n",
      "2020-10-06 15:26:42,637 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:26:42,639 EPOCH 3 done: loss 0.0546 - lr 0.0000300\n",
      "2020-10-06 15:27:05,307 DEV : loss 0.3466345965862274 - score 0.9364\n",
      "2020-10-06 15:27:06,459 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-10-06 15:27:07,377 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:27:29,304 epoch 4 - iter 43/437 - loss 0.03202130 - samples/sec: 31.97 - lr: 0.000030\n",
      "2020-10-06 15:27:50,821 epoch 4 - iter 86/437 - loss 0.03228657 - samples/sec: 32.18 - lr: 0.000030\n",
      "2020-10-06 15:28:12,355 epoch 4 - iter 129/437 - loss 0.02339181 - samples/sec: 32.17 - lr: 0.000030\n",
      "2020-10-06 15:28:34,921 epoch 4 - iter 172/437 - loss 0.01765971 - samples/sec: 32.28 - lr: 0.000030\n",
      "2020-10-06 15:28:56,306 epoch 4 - iter 215/437 - loss 0.02518034 - samples/sec: 32.40 - lr: 0.000030\n",
      "2020-10-06 15:29:17,708 epoch 4 - iter 258/437 - loss 0.02101673 - samples/sec: 32.39 - lr: 0.000030\n",
      "2020-10-06 15:29:39,211 epoch 4 - iter 301/437 - loss 0.01809795 - samples/sec: 32.26 - lr: 0.000030\n",
      "2020-10-06 15:30:00,605 epoch 4 - iter 344/437 - loss 0.01584667 - samples/sec: 32.34 - lr: 0.000030\n",
      "2020-10-06 15:30:22,117 epoch 4 - iter 387/437 - loss 0.01494282 - samples/sec: 32.21 - lr: 0.000030\n",
      "2020-10-06 15:30:43,593 epoch 4 - iter 430/437 - loss 0.01345090 - samples/sec: 32.23 - lr: 0.000030\n",
      "2020-10-06 15:30:47,191 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:30:47,192 EPOCH 4 done: loss 0.0132 - lr 0.0000300\n",
      "2020-10-06 15:31:12,135 DEV : loss 0.5440820455551147 - score 0.9373\n",
      "2020-10-06 15:31:13,317 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-10-06 15:31:15,624 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:31:37,373 epoch 5 - iter 43/437 - loss 0.03937255 - samples/sec: 32.24 - lr: 0.000030\n",
      "2020-10-06 15:32:00,039 epoch 5 - iter 86/437 - loss 0.01972234 - samples/sec: 32.02 - lr: 0.000030\n",
      "2020-10-06 15:32:21,431 epoch 5 - iter 129/437 - loss 0.01606144 - samples/sec: 32.35 - lr: 0.000030\n",
      "2020-10-06 15:32:42,959 epoch 5 - iter 172/437 - loss 0.01211421 - samples/sec: 32.16 - lr: 0.000030\n",
      "2020-10-06 15:33:04,243 epoch 5 - iter 215/437 - loss 0.00973439 - samples/sec: 32.45 - lr: 0.000030\n",
      "2020-10-06 15:33:25,556 epoch 5 - iter 258/437 - loss 0.00811542 - samples/sec: 32.44 - lr: 0.000030\n",
      "2020-10-06 15:33:46,991 epoch 5 - iter 301/437 - loss 0.01073165 - samples/sec: 32.23 - lr: 0.000030\n",
      "2020-10-06 15:34:08,390 epoch 5 - iter 344/437 - loss 0.00950970 - samples/sec: 32.27 - lr: 0.000030\n",
      "2020-10-06 15:34:31,625 epoch 5 - iter 387/437 - loss 0.00934218 - samples/sec: 31.18 - lr: 0.000030\n",
      "2020-10-06 15:34:53,968 epoch 5 - iter 430/437 - loss 0.00841678 - samples/sec: 30.99 - lr: 0.000030\n",
      "2020-10-06 15:34:57,762 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:34:57,764 EPOCH 5 done: loss 0.0083 - lr 0.0000300\n",
      "2020-10-06 15:35:20,573 DEV : loss 0.5825539231300354 - score 0.9325\n",
      "2020-10-06 15:35:21,723 BAD EPOCHS (no improvement): 1\n",
      "2020-10-06 15:35:22,071 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:35:22,072 Testing using best model ...\n",
      "2020-10-06 15:35:22,073 loading file resources/classifier/machine/best-model.pt\n",
      "2020-10-06 15:35:45,893 \t0.935\n",
      "2020-10-06 15:35:45,894 \n",
      "Results:\n",
      "- F-score (micro) 0.935\n",
      "- F-score (macro) 0.8011\n",
      "- Accuracy 0.935\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " not_machine     0.9532    0.9756    0.9643      2090\n",
      "     machine     0.7228    0.5708    0.6379       233\n",
      "\n",
      "   micro avg     0.9350    0.9350    0.9350      2323\n",
      "   macro avg     0.8380    0.7732    0.8011      2323\n",
      "weighted avg     0.9301    0.9350    0.9316      2323\n",
      " samples avg     0.9350    0.9350    0.9350      2323\n",
      "\n",
      "2020-10-06 15:35:45,895 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.935,\n",
       " 'dev_score_history': [0.9325, 0.9351, 0.9364, 0.9373, 0.9325],\n",
       " 'train_loss_history': [0.2574387380610342,\n",
       "  0.12017576302693692,\n",
       "  0.05461023818249697,\n",
       "  0.013235492687203518,\n",
       "  0.008282100172282902],\n",
       " 'dev_loss_history': [0.19068484008312225,\n",
       "  0.2303922325372696,\n",
       "  0.3466345965862274,\n",
       "  0.5440820455551147,\n",
       "  0.5825539231300354]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim.adam import Adam\n",
    "from flair.data import Corpus\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "# 3. initialize transformer document embeddings (many models are available)\n",
    "document_embeddings = TransformerDocumentEmbeddings('distilbert-base-uncased', fine_tune=True)\n",
    "\n",
    "# 4. create the text classifier\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict)\n",
    "\n",
    "# 5. initialize the text classifier trainer with Adam optimizer\n",
    "trainer = ModelTrainer(classifier, corpus, optimizer=Adam)\n",
    "\n",
    "# 6. start the training\n",
    "trainer.train(f'{dp}/resources/classifier/machine',\n",
    "              learning_rate=1e-1, # use very small learning rate\n",
    "              mini_batch_size=16,\n",
    "              mini_batch_chunk_size=4, # optionally set this if transformer is too much for your machine\n",
    "              max_epochs=5, # terminate after 5 epochs\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_for_sequence_tagging(row,mask_target=False):\n",
    "    \n",
    "    # try spaCy\n",
    "    \n",
    "    punct = [',','.',' ','?','!',\";\",\":\",\"(\",\")\"]\n",
    "    sentence = row.text[\"full_text\"]\n",
    "    \n",
    "    sentence = np.array([i for i in sentence])\n",
    "    offset = row.text[\"keyword_offset\"]\n",
    "    target = row.text[\"keyword\"]\n",
    "    \n",
    "    if not target:\n",
    "        return None\n",
    "    \n",
    "    labels = np.array([0]*len(sentence))\n",
    "    end = offset + len(target)\n",
    "    labels[offset:end] = 1\n",
    "    \n",
    "    for ch in punct:\n",
    "        labels[np.where(sentence==ch)] = 2\n",
    "    \n",
    "    rows = []\n",
    "    word,labs = [],[]\n",
    "    \n",
    "    for i in range(len(sentence)):\n",
    "        if labels[i] < 2:\n",
    "            word.append(sentence[i])\n",
    "            labs.append(labels[i])\n",
    "        \n",
    "        if labels[i] == 2 and word:\n",
    "            rows.append((''.join(word),{0:\"not_machine\",1:row.label}[list(set(labs))[0]]))\n",
    "            word,labs = [],[]\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_all[\"tagged\"] = combined_all.apply(process_for_sequence_tagging,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'not_machine'),\n",
       " ('mesuring', 'not_machine'),\n",
       " ('of', 'not_machine'),\n",
       " ('salte', 'not_machine'),\n",
       " ('and', 'not_machine'),\n",
       " ('corne', 'not_machine'),\n",
       " ('that', 'not_machine'),\n",
       " ('sholde', 'not_machine'),\n",
       " ('long', 'not_machine'),\n",
       " ('to', 'not_machine'),\n",
       " ('the', 'not_machine'),\n",
       " ('shifte', 'not_machine'),\n",
       " ('of', 'not_machine'),\n",
       " ('the', 'not_machine'),\n",
       " ('communes', 'not_machine')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_all.iloc[1001].tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_all['partition'] = [np.random.choice(['train','dev','test'], p=[0.6, 0.2, 0.2]) for _ in range(combined_all.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df2string(df):\n",
    "    return \"\\n\\n\".join(['\\n'.join(['\\t'.join(e) for e in l]) for l in df.tagged.to_list() if l])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in [\"train\",\"test\",\"dev\"]:\n",
    "    string = df2string(combined_all[combined_all['partition']==split])\n",
    "    with open(f'{dp}/{split}.csv','w') as out_doc:\n",
    "        out_doc.write(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-06 15:44:47,412 Reading data from ../data\n",
      "2020-10-06 15:44:47,413 Train: ../data/train.csv\n",
      "2020-10-06 15:44:47,413 Dev: ../data/dev.csv\n",
      "2020-10-06 15:44:47,413 Test: ../data/test.csv\n",
      "Dictionary with 6 tags: <unk>, O, not_machine, machine, <START>, <STOP>\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import ColumnCorpus\n",
    "\n",
    "# define columns\n",
    "columns = {0: 'text', 1: 'label'}\n",
    "\n",
    "\n",
    "# init a corpus using column format, data folder and the names of the train, dev and test files\n",
    "corpus: Corpus = ColumnCorpus(path, columns,\n",
    "                              train_file='train.csv',\n",
    "                              test_file='test.csv',\n",
    "                              dev_file='dev.csv')\n",
    "    \n",
    "# 3. make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type='label')\n",
    "print(tag_dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-06 15:46:05,537 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:46:05,538 Model: \"SequenceTagger(\n",
      "  (embeddings): StackedEmbeddings(\n",
      "    (list_embedding_0): WordEmbeddings('glove')\n",
      "    (list_embedding_1): FlairEmbeddings(\n",
      "      (lm): LanguageModel(\n",
      "        (drop): Dropout(p=0.05, inplace=False)\n",
      "        (encoder): Embedding(300, 100)\n",
      "        (rnn): LSTM(100, 2048)\n",
      "        (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (list_embedding_2): FlairEmbeddings(\n",
      "      (lm): LanguageModel(\n",
      "        (drop): Dropout(p=0.05, inplace=False)\n",
      "        (encoder): Embedding(300, 100)\n",
      "        (rnn): LSTM(100, 2048)\n",
      "        (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=4196, out_features=4196, bias=True)\n",
      "  (rnn): LSTM(4196, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=6, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2020-10-06 15:46:05,539 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:46:05,539 Corpus: \"Corpus: 6668 train + 2197 dev + 2139 test sentences\"\n",
      "2020-10-06 15:46:05,540 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:46:05,540 Parameters:\n",
      "2020-10-06 15:46:05,541  - learning_rate: \"0.1\"\n",
      "2020-10-06 15:46:05,541  - mini_batch_size: \"32\"\n",
      "2020-10-06 15:46:05,542  - patience: \"3\"\n",
      "2020-10-06 15:46:05,542  - anneal_factor: \"0.5\"\n",
      "2020-10-06 15:46:05,543  - max_epochs: \"150\"\n",
      "2020-10-06 15:46:05,545  - shuffle: \"True\"\n",
      "2020-10-06 15:46:05,557  - train_with_dev: \"False\"\n",
      "2020-10-06 15:46:05,558  - batch_growth_annealing: \"False\"\n",
      "2020-10-06 15:46:05,558 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:46:05,559 Model training base path: \"../data/taggers/machine\"\n",
      "2020-10-06 15:46:05,559 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:46:05,560 Device: cuda:0\n",
      "2020-10-06 15:46:05,561 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:46:05,561 Embeddings storage mode: cpu\n",
      "2020-10-06 15:46:05,563 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:46:22,608 epoch 1 - iter 20/209 - loss 4.09275543 - samples/sec: 37.55 - lr: 0.100000\n",
      "2020-10-06 15:46:39,196 epoch 1 - iter 40/209 - loss 2.33400882 - samples/sec: 38.59 - lr: 0.100000\n",
      "2020-10-06 15:46:56,993 epoch 1 - iter 60/209 - loss 1.72974047 - samples/sec: 35.97 - lr: 0.100000\n",
      "2020-10-06 15:47:15,378 epoch 1 - iter 80/209 - loss 1.45138324 - samples/sec: 34.82 - lr: 0.100000\n",
      "2020-10-06 15:47:32,740 epoch 1 - iter 100/209 - loss 1.26141423 - samples/sec: 36.87 - lr: 0.100000\n",
      "2020-10-06 15:47:50,222 epoch 1 - iter 120/209 - loss 1.13522633 - samples/sec: 36.61 - lr: 0.100000\n",
      "2020-10-06 15:48:07,485 epoch 1 - iter 140/209 - loss 1.03994858 - samples/sec: 37.08 - lr: 0.100000\n",
      "2020-10-06 15:48:25,431 epoch 1 - iter 160/209 - loss 0.97037952 - samples/sec: 35.67 - lr: 0.100000\n",
      "2020-10-06 15:48:42,487 epoch 1 - iter 180/209 - loss 0.92633456 - samples/sec: 37.53 - lr: 0.100000\n",
      "2020-10-06 15:49:00,775 epoch 1 - iter 200/209 - loss 0.86712272 - samples/sec: 35.01 - lr: 0.100000\n",
      "2020-10-06 15:49:08,471 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:49:08,472 EPOCH 1 done: loss 0.8465 - lr 0.1000000\n",
      "2020-10-06 15:50:05,428 DEV : loss 0.4454134702682495 - score 0.9929\n",
      "2020-10-06 15:50:05,598 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-10-06 15:50:08,485 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:50:12,844 epoch 2 - iter 20/209 - loss 0.43954891 - samples/sec: 146.91 - lr: 0.100000\n",
      "2020-10-06 15:50:17,332 epoch 2 - iter 40/209 - loss 0.39728476 - samples/sec: 142.69 - lr: 0.100000\n",
      "2020-10-06 15:50:21,783 epoch 2 - iter 60/209 - loss 0.42451352 - samples/sec: 143.86 - lr: 0.100000\n",
      "2020-10-06 15:50:26,208 epoch 2 - iter 80/209 - loss 0.41792257 - samples/sec: 144.69 - lr: 0.100000\n",
      "2020-10-06 15:50:30,426 epoch 2 - iter 100/209 - loss 0.42314528 - samples/sec: 151.80 - lr: 0.100000\n",
      "2020-10-06 15:50:34,767 epoch 2 - iter 120/209 - loss 0.43219602 - samples/sec: 147.49 - lr: 0.100000\n",
      "2020-10-06 15:50:39,268 epoch 2 - iter 140/209 - loss 0.42758915 - samples/sec: 142.24 - lr: 0.100000\n",
      "2020-10-06 15:50:43,238 epoch 2 - iter 160/209 - loss 0.43696084 - samples/sec: 161.29 - lr: 0.100000\n",
      "2020-10-06 15:50:47,410 epoch 2 - iter 180/209 - loss 0.44004264 - samples/sec: 153.50 - lr: 0.100000\n",
      "2020-10-06 15:50:51,652 epoch 2 - iter 200/209 - loss 0.43865113 - samples/sec: 150.93 - lr: 0.100000\n",
      "2020-10-06 15:50:53,525 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:50:53,526 EPOCH 2 done: loss 0.4429 - lr 0.1000000\n",
      "2020-10-06 15:51:01,906 DEV : loss 0.5290618538856506 - score 0.9916\n",
      "2020-10-06 15:51:02,078 BAD EPOCHS (no improvement): 1\n",
      "2020-10-06 15:51:02,079 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:51:06,890 epoch 3 - iter 20/209 - loss 0.48509279 - samples/sec: 133.10 - lr: 0.100000\n",
      "2020-10-06 15:51:11,349 epoch 3 - iter 40/209 - loss 0.45949849 - samples/sec: 143.60 - lr: 0.100000\n",
      "2020-10-06 15:51:15,755 epoch 3 - iter 60/209 - loss 0.44972619 - samples/sec: 145.32 - lr: 0.100000\n",
      "2020-10-06 15:51:20,459 epoch 3 - iter 80/209 - loss 0.42532113 - samples/sec: 136.10 - lr: 0.100000\n",
      "2020-10-06 15:51:24,888 epoch 3 - iter 100/209 - loss 0.43784056 - samples/sec: 144.59 - lr: 0.100000\n",
      "2020-10-06 15:51:29,123 epoch 3 - iter 120/209 - loss 0.43561440 - samples/sec: 151.16 - lr: 0.100000\n",
      "2020-10-06 15:51:33,390 epoch 3 - iter 140/209 - loss 0.43262471 - samples/sec: 150.09 - lr: 0.100000\n",
      "2020-10-06 15:51:37,621 epoch 3 - iter 160/209 - loss 0.42439677 - samples/sec: 151.34 - lr: 0.100000\n",
      "2020-10-06 15:51:41,776 epoch 3 - iter 180/209 - loss 0.42481642 - samples/sec: 154.07 - lr: 0.100000\n",
      "2020-10-06 15:51:46,088 epoch 3 - iter 200/209 - loss 0.42262723 - samples/sec: 148.50 - lr: 0.100000\n",
      "2020-10-06 15:51:47,888 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:51:47,889 EPOCH 3 done: loss 0.4227 - lr 0.1000000\n",
      "2020-10-06 15:51:56,461 DEV : loss 0.3890500068664551 - score 0.9929\n",
      "2020-10-06 15:51:56,628 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-10-06 15:52:02,087 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:52:06,795 epoch 4 - iter 20/209 - loss 0.40154912 - samples/sec: 136.03 - lr: 0.100000\n",
      "2020-10-06 15:52:11,186 epoch 4 - iter 40/209 - loss 0.39901522 - samples/sec: 145.81 - lr: 0.100000\n",
      "2020-10-06 15:52:15,752 epoch 4 - iter 60/209 - loss 0.41076260 - samples/sec: 140.24 - lr: 0.100000\n",
      "2020-10-06 15:52:20,154 epoch 4 - iter 80/209 - loss 0.40714034 - samples/sec: 145.44 - lr: 0.100000\n",
      "2020-10-06 15:52:24,515 epoch 4 - iter 100/209 - loss 0.40914157 - samples/sec: 146.85 - lr: 0.100000\n",
      "2020-10-06 15:52:28,952 epoch 4 - iter 120/209 - loss 0.40473419 - samples/sec: 144.29 - lr: 0.100000\n",
      "2020-10-06 15:52:33,472 epoch 4 - iter 140/209 - loss 0.40847297 - samples/sec: 141.68 - lr: 0.100000\n",
      "2020-10-06 15:52:38,054 epoch 4 - iter 160/209 - loss 0.40602383 - samples/sec: 139.74 - lr: 0.100000\n",
      "2020-10-06 15:52:42,472 epoch 4 - iter 180/209 - loss 0.40888507 - samples/sec: 144.93 - lr: 0.100000\n",
      "2020-10-06 15:52:46,837 epoch 4 - iter 200/209 - loss 0.41015901 - samples/sec: 146.86 - lr: 0.100000\n",
      "2020-10-06 15:52:48,474 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:52:48,475 EPOCH 4 done: loss 0.4084 - lr 0.1000000\n",
      "2020-10-06 15:52:57,051 DEV : loss 0.3738522529602051 - score 0.993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-06 15:52:57,220 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-10-06 15:53:04,250 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:53:09,093 epoch 5 - iter 20/209 - loss 0.36932054 - samples/sec: 132.23 - lr: 0.100000\n",
      "2020-10-06 15:53:13,664 epoch 5 - iter 40/209 - loss 0.38986864 - samples/sec: 140.08 - lr: 0.100000\n",
      "2020-10-06 15:53:17,936 epoch 5 - iter 60/209 - loss 0.40185719 - samples/sec: 150.01 - lr: 0.100000\n",
      "2020-10-06 15:53:22,189 epoch 5 - iter 80/209 - loss 0.39843519 - samples/sec: 150.55 - lr: 0.100000\n",
      "2020-10-06 15:53:26,594 epoch 5 - iter 100/209 - loss 0.39489666 - samples/sec: 145.37 - lr: 0.100000\n",
      "2020-10-06 15:53:31,003 epoch 5 - iter 120/209 - loss 0.39567720 - samples/sec: 145.22 - lr: 0.100000\n",
      "2020-10-06 15:53:35,442 epoch 5 - iter 140/209 - loss 0.37708616 - samples/sec: 144.24 - lr: 0.100000\n",
      "2020-10-06 15:53:39,988 epoch 5 - iter 160/209 - loss 0.38599806 - samples/sec: 140.82 - lr: 0.100000\n",
      "2020-10-06 15:53:44,285 epoch 5 - iter 180/209 - loss 0.39158281 - samples/sec: 149.06 - lr: 0.100000\n",
      "2020-10-06 15:53:48,571 epoch 5 - iter 200/209 - loss 0.39713229 - samples/sec: 149.39 - lr: 0.100000\n",
      "2020-10-06 15:53:50,371 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:53:50,372 EPOCH 5 done: loss 0.3942 - lr 0.1000000\n",
      "2020-10-06 15:53:58,472 DEV : loss 0.3528309464454651 - score 0.9934\n",
      "2020-10-06 15:53:58,641 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-10-06 15:54:04,318 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:54:08,535 epoch 6 - iter 20/209 - loss 0.39545687 - samples/sec: 151.87 - lr: 0.100000\n",
      "2020-10-06 15:54:12,709 epoch 6 - iter 40/209 - loss 0.38019548 - samples/sec: 153.43 - lr: 0.100000\n",
      "2020-10-06 15:54:17,179 epoch 6 - iter 60/209 - loss 0.36459893 - samples/sec: 143.22 - lr: 0.100000\n",
      "2020-10-06 15:54:21,456 epoch 6 - iter 80/209 - loss 0.36709757 - samples/sec: 149.70 - lr: 0.100000\n",
      "2020-10-06 15:54:25,825 epoch 6 - iter 100/209 - loss 0.36361117 - samples/sec: 146.57 - lr: 0.100000\n",
      "2020-10-06 15:54:30,302 epoch 6 - iter 120/209 - loss 0.36366483 - samples/sec: 143.02 - lr: 0.100000\n",
      "2020-10-06 15:54:34,676 epoch 6 - iter 140/209 - loss 0.38050464 - samples/sec: 146.39 - lr: 0.100000\n",
      "2020-10-06 15:54:39,182 epoch 6 - iter 160/209 - loss 0.38444698 - samples/sec: 142.11 - lr: 0.100000\n",
      "2020-10-06 15:54:43,713 epoch 6 - iter 180/209 - loss 0.38574757 - samples/sec: 141.29 - lr: 0.100000\n",
      "2020-10-06 15:54:47,754 epoch 6 - iter 200/209 - loss 0.38481616 - samples/sec: 158.48 - lr: 0.100000\n",
      "2020-10-06 15:54:49,672 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:54:49,673 EPOCH 6 done: loss 0.3861 - lr 0.1000000\n",
      "2020-10-06 15:54:58,872 DEV : loss 0.3910372853279114 - score 0.9926\n",
      "2020-10-06 15:54:59,041 BAD EPOCHS (no improvement): 1\n",
      "2020-10-06 15:54:59,042 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:55:03,160 epoch 7 - iter 20/209 - loss 0.34039602 - samples/sec: 155.53 - lr: 0.100000\n",
      "2020-10-06 15:55:07,399 epoch 7 - iter 40/209 - loss 0.36408612 - samples/sec: 151.05 - lr: 0.100000\n",
      "2020-10-06 15:55:11,530 epoch 7 - iter 60/209 - loss 0.38178419 - samples/sec: 154.98 - lr: 0.100000\n",
      "2020-10-06 15:55:15,940 epoch 7 - iter 80/209 - loss 0.37071893 - samples/sec: 145.19 - lr: 0.100000\n",
      "2020-10-06 15:55:20,250 epoch 7 - iter 100/209 - loss 0.36924489 - samples/sec: 148.69 - lr: 0.100000\n",
      "2020-10-06 15:55:24,498 epoch 7 - iter 120/209 - loss 0.36594439 - samples/sec: 150.71 - lr: 0.100000\n",
      "2020-10-06 15:55:28,788 epoch 7 - iter 140/209 - loss 0.36897175 - samples/sec: 149.28 - lr: 0.100000\n",
      "2020-10-06 15:55:32,997 epoch 7 - iter 160/209 - loss 0.35929393 - samples/sec: 152.11 - lr: 0.100000\n",
      "2020-10-06 15:55:37,350 epoch 7 - iter 180/209 - loss 0.36426615 - samples/sec: 147.09 - lr: 0.100000\n",
      "2020-10-06 15:55:41,346 epoch 7 - iter 200/209 - loss 0.37079133 - samples/sec: 160.22 - lr: 0.100000\n",
      "2020-10-06 15:55:43,116 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:55:43,117 EPOCH 7 done: loss 0.3694 - lr 0.1000000\n",
      "2020-10-06 15:55:51,098 DEV : loss 0.34435370564460754 - score 0.9934\n",
      "2020-10-06 15:55:51,284 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-10-06 15:55:56,774 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:56:01,010 epoch 8 - iter 20/209 - loss 0.37475684 - samples/sec: 151.19 - lr: 0.100000\n",
      "2020-10-06 15:56:05,627 epoch 8 - iter 40/209 - loss 0.35245801 - samples/sec: 138.67 - lr: 0.100000\n",
      "2020-10-06 15:56:09,987 epoch 8 - iter 60/209 - loss 0.35171642 - samples/sec: 146.86 - lr: 0.100000\n",
      "2020-10-06 15:56:14,350 epoch 8 - iter 80/209 - loss 0.35400650 - samples/sec: 146.75 - lr: 0.100000\n",
      "2020-10-06 15:56:18,759 epoch 8 - iter 100/209 - loss 0.36063456 - samples/sec: 145.25 - lr: 0.100000\n",
      "2020-10-06 15:56:22,880 epoch 8 - iter 120/209 - loss 0.36277094 - samples/sec: 155.35 - lr: 0.100000\n",
      "2020-10-06 15:56:27,196 epoch 8 - iter 140/209 - loss 0.36623995 - samples/sec: 148.36 - lr: 0.100000\n",
      "2020-10-06 15:56:31,385 epoch 8 - iter 160/209 - loss 0.36322233 - samples/sec: 152.84 - lr: 0.100000\n",
      "2020-10-06 15:56:35,802 epoch 8 - iter 180/209 - loss 0.36975994 - samples/sec: 144.99 - lr: 0.100000\n",
      "2020-10-06 15:56:40,089 epoch 8 - iter 200/209 - loss 0.36722312 - samples/sec: 149.34 - lr: 0.100000\n",
      "2020-10-06 15:56:41,980 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:56:41,985 EPOCH 8 done: loss 0.3670 - lr 0.1000000\n",
      "2020-10-06 15:56:51,145 DEV : loss 0.36366531252861023 - score 0.9931\n",
      "2020-10-06 15:56:51,317 BAD EPOCHS (no improvement): 1\n",
      "2020-10-06 15:56:51,318 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:56:56,187 epoch 9 - iter 20/209 - loss 0.42183997 - samples/sec: 131.49 - lr: 0.100000\n",
      "2020-10-06 15:57:00,607 epoch 9 - iter 40/209 - loss 0.34073225 - samples/sec: 144.87 - lr: 0.100000\n",
      "2020-10-06 15:57:05,248 epoch 9 - iter 60/209 - loss 0.34554530 - samples/sec: 137.97 - lr: 0.100000\n",
      "2020-10-06 15:57:09,862 epoch 9 - iter 80/209 - loss 0.33200349 - samples/sec: 138.80 - lr: 0.100000\n",
      "2020-10-06 15:57:14,758 epoch 9 - iter 100/209 - loss 0.33031260 - samples/sec: 130.78 - lr: 0.100000\n",
      "2020-10-06 15:57:19,045 epoch 9 - iter 120/209 - loss 0.33845471 - samples/sec: 149.37 - lr: 0.100000\n",
      "2020-10-06 15:57:23,701 epoch 9 - iter 140/209 - loss 0.36174910 - samples/sec: 137.52 - lr: 0.100000\n",
      "2020-10-06 15:57:28,113 epoch 9 - iter 160/209 - loss 0.36217771 - samples/sec: 145.12 - lr: 0.100000\n",
      "2020-10-06 15:57:32,871 epoch 9 - iter 180/209 - loss 0.36561422 - samples/sec: 134.58 - lr: 0.100000\n",
      "2020-10-06 15:57:37,746 epoch 9 - iter 200/209 - loss 0.36173372 - samples/sec: 131.33 - lr: 0.100000\n",
      "2020-10-06 15:57:39,777 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:57:39,778 EPOCH 9 done: loss 0.3591 - lr 0.1000000\n",
      "2020-10-06 15:57:48,273 DEV : loss 0.37558695673942566 - score 0.9931\n",
      "2020-10-06 15:57:48,443 BAD EPOCHS (no improvement): 2\n",
      "2020-10-06 15:57:48,444 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:57:52,880 epoch 10 - iter 20/209 - loss 0.37472244 - samples/sec: 144.34 - lr: 0.100000\n",
      "2020-10-06 15:57:57,090 epoch 10 - iter 40/209 - loss 0.34065409 - samples/sec: 152.09 - lr: 0.100000\n",
      "2020-10-06 15:58:01,378 epoch 10 - iter 60/209 - loss 0.32442527 - samples/sec: 149.33 - lr: 0.100000\n",
      "2020-10-06 15:58:05,751 epoch 10 - iter 80/209 - loss 0.33464711 - samples/sec: 146.43 - lr: 0.100000\n",
      "2020-10-06 15:58:09,851 epoch 10 - iter 100/209 - loss 0.34204630 - samples/sec: 156.18 - lr: 0.100000\n",
      "2020-10-06 15:58:14,179 epoch 10 - iter 120/209 - loss 0.35532519 - samples/sec: 147.92 - lr: 0.100000\n",
      "2020-10-06 15:58:18,264 epoch 10 - iter 140/209 - loss 0.35883629 - samples/sec: 156.76 - lr: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-06 15:58:22,371 epoch 10 - iter 160/209 - loss 0.36188491 - samples/sec: 155.90 - lr: 0.100000\n",
      "2020-10-06 15:58:26,693 epoch 10 - iter 180/209 - loss 0.35614456 - samples/sec: 148.16 - lr: 0.100000\n",
      "2020-10-06 15:58:31,218 epoch 10 - iter 200/209 - loss 0.35553813 - samples/sec: 141.51 - lr: 0.100000\n",
      "2020-10-06 15:58:33,158 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:58:33,158 EPOCH 10 done: loss 0.3541 - lr 0.1000000\n",
      "2020-10-06 15:58:41,649 DEV : loss 0.34489959478378296 - score 0.9934\n",
      "2020-10-06 15:58:41,830 BAD EPOCHS (no improvement): 3\n",
      "2020-10-06 15:58:41,831 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:58:46,595 epoch 11 - iter 20/209 - loss 0.27771169 - samples/sec: 134.41 - lr: 0.100000\n",
      "2020-10-06 15:58:51,021 epoch 11 - iter 40/209 - loss 0.27482983 - samples/sec: 144.65 - lr: 0.100000\n",
      "2020-10-06 15:58:55,582 epoch 11 - iter 60/209 - loss 0.30090500 - samples/sec: 140.40 - lr: 0.100000\n",
      "2020-10-06 15:58:59,803 epoch 11 - iter 80/209 - loss 0.30969430 - samples/sec: 151.68 - lr: 0.100000\n",
      "2020-10-06 15:59:04,160 epoch 11 - iter 100/209 - loss 0.30280899 - samples/sec: 146.98 - lr: 0.100000\n",
      "2020-10-06 15:59:08,349 epoch 11 - iter 120/209 - loss 0.32107921 - samples/sec: 152.84 - lr: 0.100000\n",
      "2020-10-06 15:59:12,653 epoch 11 - iter 140/209 - loss 0.32476416 - samples/sec: 148.76 - lr: 0.100000\n",
      "2020-10-06 15:59:16,873 epoch 11 - iter 160/209 - loss 0.33228536 - samples/sec: 151.72 - lr: 0.100000\n",
      "2020-10-06 15:59:21,021 epoch 11 - iter 180/209 - loss 0.33246746 - samples/sec: 154.52 - lr: 0.100000\n",
      "2020-10-06 15:59:25,315 epoch 11 - iter 200/209 - loss 0.33446230 - samples/sec: 149.14 - lr: 0.100000\n",
      "2020-10-06 15:59:27,066 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:59:27,067 EPOCH 11 done: loss 0.3344 - lr 0.1000000\n",
      "2020-10-06 15:59:35,590 DEV : loss 0.3167349398136139 - score 0.9937\n",
      "2020-10-06 15:59:35,755 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-10-06 15:59:42,213 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 15:59:46,880 epoch 12 - iter 20/209 - loss 0.27024518 - samples/sec: 137.20 - lr: 0.100000\n",
      "2020-10-06 15:59:51,642 epoch 12 - iter 40/209 - loss 0.29447012 - samples/sec: 134.55 - lr: 0.100000\n",
      "2020-10-06 15:59:56,140 epoch 12 - iter 60/209 - loss 0.29722342 - samples/sec: 142.36 - lr: 0.100000\n",
      "2020-10-06 16:00:00,553 epoch 12 - iter 80/209 - loss 0.30477356 - samples/sec: 145.11 - lr: 0.100000\n",
      "2020-10-06 16:00:04,899 epoch 12 - iter 100/209 - loss 0.31850654 - samples/sec: 147.35 - lr: 0.100000\n",
      "2020-10-06 16:00:09,304 epoch 12 - iter 120/209 - loss 0.31794143 - samples/sec: 145.36 - lr: 0.100000\n",
      "2020-10-06 16:00:13,757 epoch 12 - iter 140/209 - loss 0.32924511 - samples/sec: 143.82 - lr: 0.100000\n",
      "2020-10-06 16:00:18,282 epoch 12 - iter 160/209 - loss 0.33846299 - samples/sec: 141.50 - lr: 0.100000\n",
      "2020-10-06 16:00:22,654 epoch 12 - iter 180/209 - loss 0.33579122 - samples/sec: 146.47 - lr: 0.100000\n",
      "2020-10-06 16:00:26,924 epoch 12 - iter 200/209 - loss 0.33556567 - samples/sec: 149.94 - lr: 0.100000\n",
      "2020-10-06 16:00:28,701 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 16:00:28,702 EPOCH 12 done: loss 0.3349 - lr 0.1000000\n",
      "2020-10-06 16:00:37,349 DEV : loss 0.3140784204006195 - score 0.9936\n",
      "2020-10-06 16:00:37,520 BAD EPOCHS (no improvement): 1\n",
      "2020-10-06 16:00:37,521 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 16:00:42,214 epoch 13 - iter 20/209 - loss 0.28436892 - samples/sec: 136.47 - lr: 0.100000\n",
      "2020-10-06 16:00:46,491 epoch 13 - iter 40/209 - loss 0.30972900 - samples/sec: 149.68 - lr: 0.100000\n",
      "2020-10-06 16:00:50,842 epoch 13 - iter 60/209 - loss 0.30021559 - samples/sec: 147.18 - lr: 0.100000\n",
      "2020-10-06 16:00:55,199 epoch 13 - iter 80/209 - loss 0.30935162 - samples/sec: 146.96 - lr: 0.100000\n",
      "2020-10-06 16:00:59,533 epoch 13 - iter 100/209 - loss 0.32611096 - samples/sec: 147.73 - lr: 0.100000\n",
      "2020-10-06 16:01:03,784 epoch 13 - iter 120/209 - loss 0.32388523 - samples/sec: 150.61 - lr: 0.100000\n",
      "2020-10-06 16:01:07,990 epoch 13 - iter 140/209 - loss 0.32268031 - samples/sec: 152.23 - lr: 0.100000\n",
      "2020-10-06 16:01:12,149 epoch 13 - iter 160/209 - loss 0.32316766 - samples/sec: 153.97 - lr: 0.100000\n",
      "2020-10-06 16:01:16,191 epoch 13 - iter 180/209 - loss 0.31876469 - samples/sec: 158.42 - lr: 0.100000\n",
      "2020-10-06 16:01:20,495 epoch 13 - iter 200/209 - loss 0.31998218 - samples/sec: 148.80 - lr: 0.100000\n",
      "2020-10-06 16:01:22,438 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 16:01:22,439 EPOCH 13 done: loss 0.3195 - lr 0.1000000\n",
      "2020-10-06 16:01:30,765 DEV : loss 0.31143057346343994 - score 0.9939\n",
      "2020-10-06 16:01:30,933 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-10-06 16:01:38,072 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 16:01:43,962 epoch 14 - iter 20/209 - loss 0.30574853 - samples/sec: 108.71 - lr: 0.100000\n",
      "2020-10-06 16:01:48,551 epoch 14 - iter 40/209 - loss 0.29931109 - samples/sec: 139.53 - lr: 0.100000\n",
      "2020-10-06 16:01:53,096 epoch 14 - iter 60/209 - loss 0.31846728 - samples/sec: 140.88 - lr: 0.100000\n",
      "2020-10-06 16:01:57,584 epoch 14 - iter 80/209 - loss 0.31938427 - samples/sec: 142.66 - lr: 0.100000\n",
      "2020-10-06 16:02:01,851 epoch 14 - iter 100/209 - loss 0.30704288 - samples/sec: 150.08 - lr: 0.100000\n",
      "2020-10-06 16:02:06,141 epoch 14 - iter 120/209 - loss 0.30987082 - samples/sec: 149.24 - lr: 0.100000\n",
      "2020-10-06 16:02:10,233 epoch 14 - iter 140/209 - loss 0.30780691 - samples/sec: 156.49 - lr: 0.100000\n",
      "2020-10-06 16:02:14,784 epoch 14 - iter 160/209 - loss 0.31964700 - samples/sec: 140.69 - lr: 0.100000\n",
      "2020-10-06 16:02:19,061 epoch 14 - iter 180/209 - loss 0.31853402 - samples/sec: 149.71 - lr: 0.100000\n",
      "2020-10-06 16:02:23,416 epoch 14 - iter 200/209 - loss 0.31335756 - samples/sec: 147.05 - lr: 0.100000\n",
      "2020-10-06 16:02:25,307 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 16:02:25,308 EPOCH 14 done: loss 0.3156 - lr 0.1000000\n",
      "2020-10-06 16:02:33,895 DEV : loss 0.33148807287216187 - score 0.993\n",
      "2020-10-06 16:02:34,066 BAD EPOCHS (no improvement): 1\n",
      "2020-10-06 16:02:34,067 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 16:02:38,718 epoch 15 - iter 20/209 - loss 0.33049323 - samples/sec: 137.69 - lr: 0.100000\n",
      "2020-10-06 16:02:43,452 epoch 15 - iter 40/209 - loss 0.33214018 - samples/sec: 135.25 - lr: 0.100000\n",
      "2020-10-06 16:02:47,855 epoch 15 - iter 60/209 - loss 0.33168773 - samples/sec: 145.40 - lr: 0.100000\n",
      "2020-10-06 16:02:52,636 epoch 15 - iter 80/209 - loss 0.32278310 - samples/sec: 133.94 - lr: 0.100000\n",
      "2020-10-06 16:02:56,735 epoch 15 - iter 100/209 - loss 0.31330887 - samples/sec: 156.22 - lr: 0.100000\n",
      "2020-10-06 16:03:00,821 epoch 15 - iter 120/209 - loss 0.31360770 - samples/sec: 156.72 - lr: 0.100000\n",
      "2020-10-06 16:03:04,897 epoch 15 - iter 140/209 - loss 0.32374892 - samples/sec: 157.07 - lr: 0.100000\n",
      "2020-10-06 16:03:09,131 epoch 15 - iter 160/209 - loss 0.32203973 - samples/sec: 151.23 - lr: 0.100000\n",
      "2020-10-06 16:03:13,168 epoch 15 - iter 180/209 - loss 0.32038898 - samples/sec: 158.63 - lr: 0.100000\n",
      "2020-10-06 16:03:17,332 epoch 15 - iter 200/209 - loss 0.31503815 - samples/sec: 153.77 - lr: 0.100000\n",
      "2020-10-06 16:03:19,258 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 16:03:19,259 EPOCH 15 done: loss 0.3117 - lr 0.1000000\n",
      "2020-10-06 16:03:27,894 DEV : loss 0.3337770998477936 - score 0.993\n",
      "2020-10-06 16:03:28,064 BAD EPOCHS (no improvement): 2\n",
      "2020-10-06 16:03:28,065 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 16:03:32,738 epoch 16 - iter 20/209 - loss 0.29538577 - samples/sec: 137.03 - lr: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-06 16:03:34,273 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 16:03:34,274 Exiting from training early.\n",
      "2020-10-06 16:03:34,274 Saving model ...\n",
      "2020-10-06 16:03:37,136 Done.\n",
      "2020-10-06 16:03:37,137 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-06 16:03:37,138 Testing using best model ...\n",
      "2020-10-06 16:03:37,139 loading file ../data/taggers/machine/best-model.pt\n",
      "2020-10-06 16:04:18,429 \t0.994\n",
      "2020-10-06 16:04:18,431 \n",
      "Results:\n",
      "- F-score (micro) 0.994\n",
      "- F-score (macro) 0.6462\n",
      "- Accuracy 0.994\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " not_machine     0.9944    0.9996    0.9970     30579\n",
      "     machine     0.7500    0.1840    0.2955       212\n",
      "\n",
      "    accuracy                         0.9940     30791\n",
      "   macro avg     0.8722    0.5918    0.6462     30791\n",
      "weighted avg     0.9927    0.9940    0.9921     30791\n",
      "\n",
      "2020-10-06 16:04:18,431 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.994,\n",
       " 'dev_score_history': [0.9929,\n",
       "  0.9916,\n",
       "  0.9929,\n",
       "  0.993,\n",
       "  0.9934,\n",
       "  0.9926,\n",
       "  0.9934,\n",
       "  0.9931,\n",
       "  0.9931,\n",
       "  0.9934,\n",
       "  0.9937,\n",
       "  0.9936,\n",
       "  0.9939,\n",
       "  0.993,\n",
       "  0.993],\n",
       " 'train_loss_history': [0.8465260845479783,\n",
       "  0.44291141336899625,\n",
       "  0.4226515189027102,\n",
       "  0.4084052837779077,\n",
       "  0.39420261636875464,\n",
       "  0.3861422339124543,\n",
       "  0.3694431799236667,\n",
       "  0.3669909275889967,\n",
       "  0.35908130367406815,\n",
       "  0.35407543681455,\n",
       "  0.3343518824811187,\n",
       "  0.334928142183135,\n",
       "  0.3195464206797084,\n",
       "  0.3156196296785437,\n",
       "  0.3117124591575285],\n",
       " 'dev_loss_history': [0.4454134702682495,\n",
       "  0.5290618538856506,\n",
       "  0.3890500068664551,\n",
       "  0.3738522529602051,\n",
       "  0.3528309464454651,\n",
       "  0.3910372853279114,\n",
       "  0.34435370564460754,\n",
       "  0.36366531252861023,\n",
       "  0.37558695673942566,\n",
       "  0.34489959478378296,\n",
       "  0.3167349398136139,\n",
       "  0.3140784204006195,\n",
       "  0.31143057346343994,\n",
       "  0.33148807287216187,\n",
       "  0.3337770998477936]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import UD_ENGLISH\n",
    "from flair.embeddings import WordEmbeddings,FlairEmbeddings,StackedEmbeddings,TransformerWordEmbeddings\n",
    "\n",
    "\n",
    "\n",
    "# 4. initialize embeddings\n",
    "embedding_types = [\n",
    "\n",
    "    WordEmbeddings('glove'),\n",
    "\n",
    "    # comment in this line to use character embeddings\n",
    "    # CharacterEmbeddings(),\n",
    "\n",
    "    # comment in these lines to use flair embeddings\n",
    "    FlairEmbeddings('news-forward'),\n",
    "    FlairEmbeddings('news-backward'),\n",
    "]\n",
    "\n",
    "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "\n",
    "#embeddings = TransformerWordEmbeddings('bert-base-cased',fine_tune=True, allow_long_sentences=True)\n",
    "    \n",
    "# 5. initialize sequence tagger\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
    "                                        embeddings=embeddings,\n",
    "                                        tag_dictionary=tag_dictionary,\n",
    "                                        tag_type='label',\n",
    "                                        use_crf=True)\n",
    "\n",
    "# 6. initialize trainer\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "# 7. start training\n",
    "trainer.train(f'{dp}/taggers/machine',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              max_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
