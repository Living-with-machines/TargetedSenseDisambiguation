{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset_download import *\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path, PosixPath\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../oed_experiments/oed_credentials.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5bf4aa1a41d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import API credentials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../oed_experiments/oed_credentials.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcredentials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../oed_experiments/oed_credentials.json'"
     ]
    }
   ],
   "source": [
    "# import API credentials\n",
    "with open('../oed_experiments/oed_credentials.json') as f:\n",
    "    credentials = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lemma\n",
    "lemma_id = \"machine_nn01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path(dp)\n",
    "save_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query the API and get the json response\n",
    "sense_json = query_oed(credentials,'word',lemma_id,flags='include_senses=true&include_quotations=true')\n",
    "\n",
    "# convert the json in a dataframe\n",
    "senses_df = convert_json_to_dataframe(sense_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe\n",
    "# as pickle\n",
    "senses_df.to_pickle(save_path / f\"senses_{lemma_id}.pickle\")\n",
    "# as csv\n",
    "senses_df.to_csv(save_path / f\"senses_{lemma_id}.tsv\",sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open pickle file to avoid calling the API again\n",
    "with open(save_path / f\"senses_{lemma_id}.pickle\",'rb') as in_pickle:\n",
    "    machine_senses_df = pickle.load(in_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all senses that are siblings and descendants\n",
    "# of the semantic class of senses listed in previously obtained query \n",
    "responses = traverse_thesaurus(credentials,machine_senses_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traverse tree or load responses \n",
    "# responses = traverse_thesaurus(credentials,machine_senses_df)\n",
    "with open(f'{dp}/tree_traversal.pickle','rb') as in_pickle:\n",
    "    responses = pickle.load(in_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all quoations for the senses in the responses variable\n",
    "quotations = get_quotations_from_thesaurus(credentials,responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge and save all information stored in the seperate pickle files\n",
    "df = merge_pickled(Path(\"./data/senses_machine_nn01.pickle\"),\n",
    "                   Path(\"./data/tree_traversal.pickle\"),\n",
    "                   Path(\"./data/tree_traversal_quotations.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(f\"{dp}/{lemma_id}_all.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(f\"{dp}/{lemma_id}_all.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1-dataframe\u001b[m\r\n",
      "  19-machine-tagger\u001b[m\r\n",
      "  3-group-senses\u001b[m\r\n",
      "* \u001b[32m4-semantic-provenance\u001b[m\r\n",
      "  dev\u001b[m\r\n",
      "  master\u001b[m\r\n",
      "  oed-experiments\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!git branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start,end = 1750,1950\n",
    "lemma_id = 'machine_nn01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_last_id = lambda nested_list :[l[-1] for l in nested_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_from_lemma_query(auth,lemma_id,start=1750,end=1950):\n",
    "    \"\"\"Extends senses from a dataframe generate from accessing\n",
    "    the API via the word endpoint. The script first retrieves all\n",
    "    senses, then synonyms for these senses, then other senses that \n",
    "    match the semantic classes of the retrieved senses.\n",
    "    \n",
    "    This script also aims to record the \"provenance\" of words, \n",
    "    their relation to the initial query, which can help to \n",
    "    select of filter words later on.\n",
    "    \n",
    "    Arguments:\n",
    "        lemma_id (str)\n",
    "        start (int)\n",
    "        end (int)\n",
    "    Returns\n",
    "        a pandas.DataFrame\n",
    "    \"\"\"\n",
    "    # load seed query dataframe\n",
    "    query_df = pd.read_pickle(f\"./data/senses_{lemma_id}.pickle\")\n",
    "    \n",
    "    # use the sense endpoint to ensure all information \n",
    "    # can be properly concatenated in one dataframe\n",
    "    \n",
    "    # retrieve all sense ids\n",
    "    query_sense_ids = query_df.id.unique()\n",
    "    \n",
    "    # get all senses by sense id\n",
    "    print(f\"Get all sense for the lemma {lemma_id}\")\n",
    "    seeds = [(s,query_oed(auth,'sense',s,\n",
    "                    flags=f\"current_in='{start}-{end}'&limit=1000\"))\n",
    "                        for s in tqdm(query_sense_ids)]\n",
    "    \n",
    "    # convert to dataframe\n",
    "    seeds_df = pd.DataFrame([seed['data'] for s_id,seed in seeds])\n",
    "    \n",
    "    # define provenance, these words are \"seed\"\n",
    "    seeds_df['provenance'] = seeds_df.id\n",
    "    seeds_df['provenance_type'] = 'seed'\n",
    "    \n",
    "    # get all synonyms for the seed senses\n",
    "    print(f\"Get all synonyms of the senses listed in {lemma_id}\")\n",
    "    synonyms = [(s,query_oed(auth,'sense',s,\n",
    "                    level='synonyms',\n",
    "                    flags=f\"current_in='{start}-{end}'&limit=1000\"))\n",
    "                            for s in tqdm(query_sense_ids)]\n",
    "\n",
    "    # transform list of synonyms to a dataframe\n",
    "    synonyms_df = pd.DataFrame([s for s_id,syn in synonyms for s in syn['data']])\n",
    "    \n",
    "    # these items have provenancy type \"synonym\"\n",
    "    synonyms_df['provenance'] = [s_id for s_id,syn in synonyms for s in syn['data']]\n",
    "    synonyms_df['provenance_type'] = 'synonym'\n",
    "    \n",
    "    # seed + synonyms constitute the nucleas of our query\n",
    "    # branch from there\n",
    "    core_df = pd.concat([seeds_df,synonyms_df])\n",
    "    core_df['semantic_class_last_id'] = core_df['semantic_class_ids'].apply(get_last_id)\n",
    "    \n",
    "    # retrieve all semantic class ids for the senses so far\n",
    "    semantic_class_ids = set([s for l in core_df.semantic_class_last_id.to_list() for s in l])\n",
    "\n",
    "    # get all the branches for the retrieve semantic class ids\n",
    "    print(\"Get all branches for seed senses and synonyms\")\n",
    "    branches = [(idx,query_oed(auth,'semanticclass', idx, \n",
    "                        level='branchsenses',\n",
    "                        flags=f\"current_in='{start}-{end}'&limit=1000\"))\n",
    "                            for idx in tqdm(semantic_class_ids)]\n",
    "    \n",
    "    # convert API response to dataframe\n",
    "    branches_df = pd.DataFrame([s for idx,branch in branches for s in branch['data']])\n",
    "    \n",
    "    # provenance_type is branch with semantic class id \n",
    "    # that was use for retrieving the sense is the provenance\n",
    "    branches_df['provenance'] = [idx for idx,branch in branches for s in branch['data']]\n",
    "    branches_df['provenance_type'] = 'branch'\n",
    "    \n",
    "    branches_df['semantic_class_last_id'] = branches_df.semantic_class_ids.apply(get_last_id)\n",
    "    \n",
    "    # remove senses that already appear in the core_df\n",
    "    branches_df_red = branches_df.loc[~branches_df.id.isin(core_df.id)]\n",
    "    \n",
    "    # concatenate core and branch senses\n",
    "    extended_df = pd.concat([core_df,branches_df_red])\n",
    "    \n",
    "    # refine the provenance type\n",
    "    # if the last semantic class id is not equal to provenance\n",
    "    # this row is a child or descendant\n",
    "    check_membership = lambda row : row.provenance in row.semantic_class_last_id\n",
    "    extended_df.loc[(~extended_df.apply(check_membership,axis=1)) & (extended_df.provenance_type=='branch'),\n",
    "                [\"provenance_type\"]]  = \"branch_descendant\"\n",
    "    \n",
    "    # save information\n",
    "    extended_df.to_pickle(f'{dp}/senses_{lemma_id}_extended.pickle')\n",
    "    return extended_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_df = extend_from_lemma_query(credentials,lemma_id,start,end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_df.head(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
